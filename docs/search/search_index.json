{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Welcome to DS-Tools","text":"<p>A comprehensive Python library with helper functions to accelerate and simplify various stages of the data science research cycle.</p> <p>This toolkit is built on top of popular libraries like Pandas, Polars, Scikit-learn, and Matplotlib, providing a higher-level API for common tasks in Exploratory Data Analysis (EDA), feature preprocessing, model evaluation, and synthetic data generation.</p>"},{"location":"#key-features","title":"Key Features","text":"<ul> <li>Advanced Data Analysis: Get quick and detailed statistics.</li> <li>Powerful Visualizations: Generate insightful correlation and confusion matrices.</li> <li>Comprehensive Model Evaluation: Calculate a wide range of classification metrics.</li> <li>Synthetic Data Generation: Create datasets with specific statistical properties.</li> </ul> <p>Check out the API Reference for detailed information on all available functions.</p>"},{"location":"api/ds_tool/","title":"Main Toolkit","text":"<p>Data Science Tools for research and analysis.</p>"},{"location":"api/ds_tool/#ds_tool.DSTools--agenda","title":"Agenda:","text":"function_list <p>Prints the list of available tools</p> compute_metrics <p>Calculate main pre-selected classification metrics</p> corr_matrix <p>Calculate and visualize correlation matrix</p> category_stats <p>Calculate and print categorical statistics (unique values analysis)</p> sparse_calc <p>Calculate sparsity level as coefficient</p> trials_res_df <p>Aggregate Optuna optimization trials as DataFrame</p> labeling <p>Encode categorical variables with optional ordering</p> remove_outliers_iqr <p>Remove outliers using IQR method</p> stat_normal_testing <p>Perform D'Agostino's K\u00b2 test for normality</p> test_stationarity <p>Perform Dickey-Fuller test for stationarity</p> check_NINF <p>Check for NaN and infinite values in DataFrame</p> df_stats <p>Quick overview of DataFrame structure</p> describe_categorical <p>Detailed description of categorical columns</p> describe_numeric <p>Detailed description of numerical columns</p> generate_distribution <p>Generate synthetic numerical distribution with specific statistical properties</p> validate_moments <p>Helper method to check if the requested statistical moments are physically possible</p> evaluate_classification <p>Calculates, prints, and visualizes metrics for a binary classification model</p> grubbs_test <p>Performs Grubbs' test to identify a single outlier in a dataset</p> plot_confusion_matrix <p>Plots a clear and readable confusion matrix using seaborn</p> add_missing_value_features <p>Adds features based on the count of missing values per row</p> chatterjee_correlation <p>Calculates Chatterjee's rank correlation coefficient (Xi) between two variables.</p> calculate_entropy <p>Calculates the Shannon entropy of a probability distribution.</p> calculate_kl_divergence <p>Calculates the Kullback-Leibler (KL) divergence between two probability distributions.</p> min_max_scale <p>Scales specified columns of a DataFrame to the range [0, 1].</p> save_dataframes_to_zip <p>Saves one or more Pandas/Polars DataFrames into a single ZIP archive.</p> read_dataframes_from_zip <p>Reads one or more Pandas/Polars DataFrames from a ZIP archive.</p> generate_alphanum_codes <p>Generates an array of random alphanumeric codes of a specified length.</p> generate_distribution_from_metrics <p>Generates a synthetic distribution of numbers matching given statistical metrics.</p> Source code in <code>src\\ds_tool.py</code> <pre><code>class DSTools:\n    \"\"\"Data Science Tools for research and analysis.\n\n    Agenda:\n    -------\n\n    function_list:\n        Prints the list of available tools\n\n    compute_metrics:\n        Calculate main pre-selected classification metrics\n\n    corr_matrix:\n        Calculate and visualize correlation matrix\n\n    category_stats:\n        Calculate and print categorical statistics (unique values analysis)\n\n    sparse_calc:\n        Calculate sparsity level as coefficient\n\n    trials_res_df:\n        Aggregate Optuna optimization trials as DataFrame\n\n    labeling:\n        Encode categorical variables with optional ordering\n\n    remove_outliers_iqr:\n        Remove outliers using IQR method\n\n    stat_normal_testing:\n        Perform D'Agostino's K\u00b2 test for normality\n\n    test_stationarity:\n        Perform Dickey-Fuller test for stationarity\n\n    check_NINF:\n        Check for NaN and infinite values in DataFrame\n\n    df_stats:\n        Quick overview of DataFrame structure\n\n    describe_categorical:\n        Detailed description of categorical columns\n\n    describe_numeric:\n        Detailed description of numerical columns\n\n    generate_distribution:\n        Generate synthetic numerical distribution with specific statistical properties\n\n    validate_moments:\n        Helper method to check if the requested statistical moments are physically possible\n\n    evaluate_classification:\n        Calculates, prints, and visualizes metrics for a binary classification model\n\n    grubbs_test:\n        Performs Grubbs' test to identify a single outlier in a dataset\n\n    plot_confusion_matrix:\n        Plots a clear and readable confusion matrix using seaborn\n\n    add_missing_value_features:\n        Adds features based on the count of missing values per row\n\n    chatterjee_correlation:\n        Calculates Chatterjee's rank correlation coefficient (Xi) between two variables.\n\n    calculate_entropy:\n        Calculates the Shannon entropy of a probability distribution.\n\n    calculate_kl_divergence:\n        Calculates the Kullback-Leibler (KL) divergence between two probability distributions.\n\n    min_max_scale:\n        Scales specified columns of a DataFrame to the range [0, 1].\n\n    save_dataframes_to_zip:\n        Saves one or more Pandas/Polars DataFrames into a single ZIP archive.\n\n    read_dataframes_from_zip:\n        Reads one or more Pandas/Polars DataFrames from a ZIP archive.\n\n    generate_alphanum_codes:\n        Generates an array of random alphanumeric codes of a specified length.\n\n    generate_distribution_from_metrics:\n        Generates a synthetic distribution of numbers matching given statistical metrics.\n\n    \"\"\"\n\n    def __init__(self):\n        \"\"\"Initialize the DSTools class with default configurations.\"\"\"\n\n        plt.rcParams[\"figure.figsize\"] = (15, 9)\n        pd.options.display.float_format = \"{:.2f}\".format\n        np.set_printoptions(suppress=True, precision=4)\n\n        # Set random seeds for reproducibility\n        random_seed = 42\n        np.random.seed(random_seed)\n        random.seed(random_seed)\n\n        # Theme configuration\n        self.plotly_theme = \"plotly_dark\"\n\n    def function_list(self) -&gt; pd.DataFrame:\n        \"\"\"\n        Parses the list of available tools (the 'Agenda') from the class\n        docstring as a formatted table (Pandas DataFrame).\n\n        Returns:\n        pd.DataFrame: A DataFrame with 'Function Name' and 'Description'\n                      columns. Returns an empty DataFrame if the 'Agenda'\n                      section is not found.\n\n        Usage:\n            pd.set_option('display.max_colwidth', 200)\n            tools = DSTools()\n            tools.function_list()\n        \"\"\"\n        # 1. Get the main docstring of the class\n        doc = self.__class__.__doc__\n\n        if not doc:\n            print(\"Warning: No documentation found for this class.\")\n            return pd.DataFrame(columns=[\"Function Name\", \"Description\"])\n\n        # 2. Find the 'Agenda' section\n        match = re.search(r\"Agenda:\\s*---+\\s*(.*)\", doc, re.S)\n\n        if not match:\n            print(\"Warning: No 'Agenda' section found in the class documentation.\")\n            return pd.DataFrame(columns=[\"Function Name\", \"Description\"])\n\n        # 3. Parse the content with the robust regex method\n        agenda_content = match.group(1).strip()\n        lines = agenda_content.split(\"\\n\")\n\n        tools_data = []\n        current_entry = None\n        entry_pattern = re.compile(r\"^\\s*([a-zA-Z0-9_]+):\\s*(.*)\")\n\n        for line in lines:\n            m = entry_pattern.match(line)\n            if m:\n                if current_entry:\n                    current_entry[\"Description\"] = \" \".join(\n                        current_entry[\"Description\"]\n                    ).strip()\n                    tools_data.append(current_entry)\n\n                func_name = m.group(1)\n                desc_part = m.group(2).strip()\n                current_entry = {\n                    \"Function Name\": func_name,\n                    \"Description\": [desc_part] if desc_part else [],\n                }\n            elif current_entry and line.strip():\n                current_entry[\"Description\"].append(line.strip())\n\n        if current_entry:\n            current_entry[\"Description\"] = \" \".join(\n                current_entry[\"Description\"]\n            ).strip()\n            tools_data.append(current_entry)\n\n        # 4. Create and return the DataFrame\n        if not tools_data:\n            print(\"Warning: No tools found in the Agenda.\")\n            return pd.DataFrame(columns=[\"Function Name\", \"Description\"])\n        out_df = pd.DataFrame(tools_data).iloc[1:]\n\n        with pd.option_context(\n            \"display.max_colwidth\",\n            200,  # \u041c\u0430\u043a\u0441. \u0448\u0438\u0440\u0438\u043d\u0430 \u043a\u043e\u043b\u043e\u043d\u043a\u0438 (\u0432 \u0441\u0438\u043c\u0432\u043e\u043b\u0430\u0445)\n            \"display.width\",\n            350,  # \u041e\u0431\u0449\u0430\u044f \u0448\u0438\u0440\u0438\u043d\u0430 \u0432\u044b\u0432\u043e\u0434\u0430\n            \"display.colheader_justify\",\n            \"center\",  # \u0412\u044b\u0440\u0430\u0432\u043d\u0438\u0432\u0430\u043d\u0438\u0435 \u0437\u0430\u0433\u043e\u043b\u043e\u0432\u043a\u043e\u0432 \u043f\u043e \u043b\u0435\u0432\u043e\u043c\u0443 \u043a\u0440\u0430\u044e\n        ):\n            return out_df\n\n    def compute_metrics(\n        self,\n        y_true: np.ndarray,\n        y_predict: np.ndarray,\n        y_predict_proba: np.ndarray,\n        config: Optional[MetricsConfig] = None,\n    ) -&gt; pd.DataFrame:\n        \"\"\"\n        Calculate main pre-selected classification metrics.\n\n        Args:\n            y_true: True labels\n            y_predict: Predicted labels\n            y_predict_proba: Predicted probabilities\n            config: Configuration for metrics computation\n\n        Returns:\n            DataFrame with calculated metrics\n\n        Usage:\n            from ds_tool import DSTools, MetricsConfig\n            tools = DSTools()\n            metrics = tools.compute_metrics(y_test, y_pred, y_pred_proba)\n        \"\"\"\n        if config is None:\n            config = MetricsConfig()\n\n        metrics_dict = {}\n\n        # Average Precision Score\n        aps = average_precision_score(y_true, y_predict_proba) * 100\n        metrics_dict[\"Average_precision, %\"] = round(aps, 2)\n\n        if config.print_values:\n            print(f\"Average_precision = {aps:.3f} %\")\n\n        # Balanced Accuracy Score\n        bas = balanced_accuracy_score(y_true, y_predict) * 100\n        metrics_dict[\"Balanced_accuracy, %\"] = round(bas, 2)\n\n        if config.print_values:\n            print(f\"Balanced_accuracy = {bas:.3f} %\")\n\n        # Likelihood Ratios\n        clr = class_likelihood_ratios(y_true, y_predict)\n        metrics_dict[\"Likelihood_ratios+\"] = clr[0]\n        metrics_dict[\"Likelihood_ratios-\"] = clr[1]\n\n        if config.print_values:\n            print(\n                f\"Likelihood_ratios+ = {clr[0]:.3f}\\nLikelihood_ratios- = {clr[1]:.3f}\"\n            )\n\n        # Cohen's Kappa Score\n        cks = cohen_kappa_score(y_true, y_predict) * 100\n        metrics_dict[\"Kappa_score, %\"] = round(cks, 2)\n\n        if config.print_values:\n            print(f\"Kappa_score = {cks:.3f} %\")\n\n        # Hamming Loss\n        hl = hamming_loss(y_true, y_predict) * 100\n        metrics_dict[\"Incor_pred_labels (hamming_loss), %\"] = round(hl, 2)\n\n        if config.print_values:\n            print(f\"Incor_pred_labels (hamming_loss) = {hl:.3f} %\")\n\n        # Jaccard Score\n        hs = jaccard_score(y_true, y_predict) * 100\n        metrics_dict[\"Jaccard_similarity, %\"] = round(hs, 2)\n\n        if config.print_values:\n            print(f\"Jaccard_similarity = {hs:.3f} %\")\n\n        # Log Loss\n        ls = log_loss(y_true, y_predict_proba)\n        metrics_dict[\"Cross_entropy_loss\"] = ls\n\n        if config.print_values:\n            print(f\"Cross_entropy_loss = {ls:.3f}\")\n\n        # Correlation Coefficient\n        cc = np.corrcoef(y_true, y_predict)[0][1] * 100\n        metrics_dict[\"Coef_correlation, %\"] = round(cc, 2)\n\n        if config.print_values:\n            print(f\"Coef_correlation = {cc:.3f} %\")\n\n        # Error visualization\n        if config.error_vis:\n            fpr, fnr, thresholds = det_curve(y_true, y_predict_proba)\n            plt.plot(thresholds, fpr, label=\"False Positive Rate (FPR)\")\n            plt.plot(thresholds, fnr, label=\"False Negative Rate (FNR)\")\n            plt.title(\"Error Rates vs Threshold Levels\")\n            plt.xlabel(\"Threshold Level\")\n            plt.ylabel(\"Error Rate\")\n            plt.legend()\n            plt.grid(True)\n            plt.show()\n\n        return pd.DataFrame([metrics_dict])\n\n    def corr_matrix(\n        self, df: pd.DataFrame, config: Optional[CorrelationConfig] = None\n    ) -&gt; None:\n        \"\"\"\n        Calculate and visualize correlation matrix.\n\n        Args:\n            df: Input DataFrame with numerical columns\n            config: Configuration for correlation matrix visualization\n\n        Usage:\n             from ds_tool import DSTools, CorrelationConfig\n             tools = DSTools()\n             tools.corr_matrix(df, CorrelationConfig(font_size=12))\n        \"\"\"\n        if config is None:\n            config = CorrelationConfig()\n\n        # Calculate correlation matrix\n        corr = df.corr(method=config.build_method)\n        mask = np.triu(np.ones_like(corr, dtype=bool))\n\n        # Determine figure size based on number of columns\n        n_cols = len(df.columns)\n\n        if n_cols &lt; 5:\n            fig_size = (8, 8)\n        elif n_cols &lt; 9:\n            fig_size = (10, 10)\n        elif n_cols &lt; 15:\n            fig_size = (22, 22)\n        else:\n            fig_size = config.image_size\n\n        fig, ax = plt.subplots(figsize=fig_size)\n\n        # Create heatmap\n        ax = sns.heatmap(\n            corr,\n            annot=True,\n            annot_kws={\"size\": config.font_size},\n            fmt=\".3f\",\n            center=0,\n            linewidths=1.0,\n            linecolor=\"black\",\n            square=True,\n            cmap=sns.diverging_palette(20, 220, n=100),\n            mask=mask,\n        )\n\n        # Customize x-axis\n        ax.tick_params(\n            axis=\"x\",\n            which=\"major\",\n            direction=\"inout\",\n            length=20,\n            width=4,\n            color=\"m\",\n            pad=10,\n            labelsize=16,\n            labelcolor=\"b\",\n            bottom=True,\n            top=True,\n            labelbottom=True,\n            labeltop=True,\n            labelrotation=85,\n        )\n\n        # Customize y-axis\n        ax.tick_params(\n            axis=\"y\",\n            which=\"major\",\n            direction=\"inout\",\n            length=20,\n            width=4,\n            color=\"m\",\n            pad=10,\n            labelsize=16,\n            labelcolor=\"r\",\n            left=True,\n            right=False,\n            labelleft=True,\n            labelright=False,\n            labelrotation=0,\n        )\n\n        ax.set_yticklabels(\n            ax.get_yticklabels(), rotation=0, fontsize=16, verticalalignment=\"center\"\n        )\n\n        plt.title(\n            f\"Correlation ({config.build_method}) matrix for selected features\",\n            fontsize=20,\n        )\n        plt.tight_layout()\n        plt.show()\n\n    def category_stats(self, df: pd.DataFrame, col_name: str) -&gt; None:\n        \"\"\"\n        Calculate and print categorical statistics for unique values analysis.\n\n        Args:\n            df: Input DataFrame\n            col_name: Column name for statistics calculation\n\n        Usage:\n             tools = DSTools()\n             tools.category_stats(df, 'category_column')\n        \"\"\"\n        if col_name not in df.columns:\n            raise ValueError(f\"Column {col_name} not found in DataFrame\")\n\n        value_counts = df[col_name].value_counts()\n        percentage = df[col_name].value_counts(normalize=True) * 100\n\n        aggr_stats = pd.DataFrame(\n            {\n                \"uniq_names\": value_counts.index.tolist(),\n                \"amount_values\": value_counts.values.tolist(),\n                \"percentage\": percentage.values.tolist(),\n            }\n        )\n\n        aggr_stats.columns = pd.MultiIndex.from_product(\n            [[col_name], aggr_stats.columns]\n        )\n        print(aggr_stats)\n\n    def sparse_calc(self, df: pd.DataFrame) -&gt; float:\n        \"\"\"\n        Calculate sparsity level as coefficient.\n\n        Args:\n            df: Input DataFrame\n\n        Returns:\n            Sparsity coefficient as percentage\n\n        Usage:\n             tools = DSTools()\n             sparsity = tools.sparse_calc(df)\n        \"\"\"\n        sparse_coef = round(df.apply(pd.arrays.SparseArray).sparse.density * 100, 2)\n        print(f\"Level of sparsity = {sparse_coef} %\")\n\n        return sparse_coef\n\n    def trials_res_df(self, study_trials: List[Any], metric: str) -&gt; pd.DataFrame:\n        \"\"\"\n        Aggregate Optuna optimization trials as DataFrame.\n\n        Args:\n            study_trials: List of Optuna trials (study.trials)\n            metric: Metric name for sorting (e.g., 'MCC', 'F1')\n\n        Returns:\n            DataFrame with aggregated trial results\n\n        Usage:\n             tools = DSTools()\n             results = tools.trials_res_df(study.trials, 'MCC')\n        \"\"\"\n        df_results = pd.DataFrame()\n\n        for trial in study_trials:\n            if trial.value is None:\n                continue\n\n            trial_data = pd.DataFrame.from_dict(trial.params, orient=\"index\").T\n            trial_data.insert(0, metric, trial.value)\n\n            if trial.datetime_complete and trial.datetime_start:\n                duration = (\n                    trial.datetime_complete - trial.datetime_start\n                ).total_seconds()\n                trial_data[\"Duration\"] = duration\n\n            df_results = pd.concat([df_results, trial_data], ignore_index=True)\n\n        df_results = df_results.sort_values(metric, ascending=False)\n\n        for col in df_results.columns:\n            if col not in [metric, \"Duration\"]:\n                df_results[col] = pd.to_numeric(df_results[col], errors=\"coerce\")\n\n        return df_results\n\n    def labeling(\n        self, df: pd.DataFrame, col_name: str, order_flag: bool = True\n    ) -&gt; pd.DataFrame:\n        \"\"\"\n        Encode categorical variables with optional ordering.\n\n        Args:\n            df: Input DataFrame\n            col_name: Column name for transformation\n            order_flag: Whether to apply ordering based on frequency\n\n        Returns:\n            DataFrame with encoded column\n\n        Usage:\n             tools = DSTools()\n             df = tools.labeling(df, 'category_column', True)\n        \"\"\"\n        if col_name not in df.columns:\n            raise ValueError(f\"Column {col_name} not found in DataFrame\")\n\n        df_copy = df.copy()\n        unique_values = df_copy[col_name].unique()\n        value_index = dict(zip(unique_values, range(len(unique_values))))\n        print(f\"Set of unique indexes for &lt;{col_name}&gt;:\\n{value_index}\")\n\n        if order_flag:\n            counts = (\n                df_copy[col_name]\n                .value_counts(normalize=True)\n                .sort_values()\n                .index.tolist()\n            )\n            counts_dict = {val: i for i, val in enumerate(counts)}\n            encoder = OrdinalEncoder(categories=[list(counts_dict.keys())], dtype=int)\n        else:\n            encoder = OrdinalEncoder(dtype=int)\n\n        df_copy[col_name] = encoder.fit_transform(df_copy[[col_name]])\n        return df_copy\n\n    def remove_outliers_iqr(\n        self, df: pd.DataFrame, column_name: str, config: Optional[OutlierConfig] = None\n    ) -&gt; Union[pd.DataFrame, Tuple[pd.DataFrame, float, float]]:\n        \"\"\"\n        Remove outliers using IQR (Inter Quartile Range) method.\n\n        Args:\n            df: Input DataFrame\n            column_name: Target column name\n            config: Configuration for outlier removal\n\n        Returns:\n            Modified DataFrame, optionally with outlier percentages\n\n        Usage:\n             from ds_tool import DSTools, OutlierConfig\n             tools = DSTools()\n             config_custom = OutlierConfig(sigma=1.0, percentage=False)\n             df_clean = tools.remove_outliers_iqr(df, 'target_column', config=config_custom)\n             df_replaced, p_upper, p_lower = tools.remove_outliers_iqr(df, 'target_column')\n        \"\"\"\n        if config is None:\n            config = OutlierConfig()\n\n        if column_name not in df.columns:\n            raise ValueError(f\"Column {column_name} not found in DataFrame\")\n\n        df_copy = df.copy()\n        target = df_copy[column_name]\n\n        q1 = target.quantile(0.25)\n        q3 = target.quantile(0.75)\n        iqr = q3 - q1\n        iqr_lower = q1 - config.sigma * iqr\n        iqr_upper = q3 + config.sigma * iqr\n\n        outliers_upper = target &gt; iqr_upper\n        outliers_lower = target &lt; iqr_lower\n\n        if config.change_remove:\n            df_copy.loc[outliers_upper, column_name] = iqr_upper\n            df_copy.loc[outliers_lower, column_name] = iqr_lower\n        else:\n            df_copy = df_copy[~(outliers_upper | outliers_lower)]\n\n        if config.percentage:\n            percent_upper = round(outliers_upper.sum() / len(df) * 100, 2)\n            percent_lower = round(outliers_lower.sum() / len(df) * 100, 2)\n            return df_copy, percent_upper, percent_lower\n\n        return df_copy\n\n    def stat_normal_testing(\n        self, check_object: Union[pd.DataFrame, pd.Series], describe_flag: bool = False\n    ) -&gt; None:\n        \"\"\"\n        Perform D'Agostino's K\u00b2 test for normality testing.\n\n        Args:\n            check_object: Input data (DataFrame or Series)\n            describe_flag: Whether to show descriptive statistics\n\n        Usage:\n             tools = DSTools()\n\n             tools.stat_normal_testing(data, describe_flag=True)\n        \"\"\"\n        if isinstance(check_object, pd.DataFrame) and len(check_object.columns) == 1:\n            check_object = check_object.iloc[:, 0]\n\n        # Perform normality test\n        stat, p_value = stats.normaltest(check_object)\n        print(f\"Statistics = {stat:.3f}, p = {p_value:.3f}\")\n\n        alpha = 0.05\n        if p_value &gt; alpha:\n            print(\"Data looks Gaussian (fail to reject H0). Data is normal\")\n        else:\n            print(\"Data does not look Gaussian (reject H0). Data is not normal\")\n\n        # Calculate kurtosis and skewness\n        kurtosis_val = stats.kurtosis(check_object)\n        skewness_val = stats.skew(check_object)\n\n        print(f\"\\nKurtosis: {kurtosis_val:.3f}\")\n        if kurtosis_val &gt; 0:\n            print(\"Distribution has heavier tails than normal\")\n        elif kurtosis_val &lt; 0:\n            print(\"Distribution has lighter tails than normal\")\n        else:\n            print(\"Distribution has normal tail weight\")\n\n        print(f\"\\nSkewness: {skewness_val:.3f}\")\n        if -0.5 &lt;= skewness_val &lt;= 0.5:\n            print(\"Data are fairly symmetrical\")\n        elif skewness_val &lt; -1 or skewness_val &gt; 1:\n            print(\"Data are highly skewed\")\n        else:\n            print(\"Data are moderately skewed\")\n\n        # Visualization\n        sns.displot(check_object, bins=30)\n        plt.title(\"Distribution of the data\")\n        plt.show()\n\n        if describe_flag:\n            print(\"\\nDescriptive Statistics:\")\n            print(check_object.describe())\n\n            fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 6))\n\n            ax1.hist(check_object, bins=50, edgecolor=\"black\")\n            ax1.set_title(\"Histogram\")\n            ax1.set_xlabel(\"Values\")\n            ax1.set_ylabel(\"Frequency\")\n\n            stats.probplot(check_object, dist=\"norm\", plot=ax2)\n            ax2.set_title(\"Q-Q Plot\")\n\n            plt.tight_layout()\n            plt.show()\n\n    def test_stationarity(\n        self,\n        check_object: pd.Series,\n        print_results_flag: bool = True,\n        len_window: int = 30,\n    ) -&gt; None:\n        \"\"\"\n        Perform Dickey-Fuller test for stationarity testing.\n\n        Args:\n            check_object: Input time series data\n            print_results_flag: Whether to print detailed results\n            len_window: length of a window, default is 30\n\n        Usage:\n             tools = DSTools()\n             tools.test_stationarity(time_series, print_results_flag=True)\n        \"\"\"\n        # Calculate rolling statistics\n        rolling_mean = check_object.rolling(window=len_window).mean()\n        rolling_std = check_object.rolling(window=len_window).std()\n\n        # Plot rolling statistics\n        fig, ax = plt.subplots(figsize=(12, 6))\n        ax.plot(check_object, color=\"blue\", label=\"Original\", linewidth=2)\n        ax.plot(rolling_mean, color=\"red\", label=\"Rolling Mean\", linewidth=2)\n        ax.plot(rolling_std, color=\"black\", label=\"Rolling Std\", linewidth=2)\n\n        ax.legend(loc=\"upper left\")\n        ax.set_title(\"Rolling Mean &amp; Standard Deviation\")\n        ax.grid(True, alpha=0.3)\n        plt.tight_layout()\n        plt.show()\n        plt.close(fig)\n\n        # Perform Dickey-Fuller test\n        adf_result = adfuller(check_object, autolag=\"AIC\")\n        adf_output = pd.Series(\n            adf_result[0:4],\n            index=[\n                \"Test Statistic\",\n                \"p-value\",\n                \"Lags Used\",\n                \"Number of Observations Used\",\n            ],\n        )\n\n        for key, value in adf_result[4].items():\n            adf_output[f\"Critical Value ({key})\"] = value\n\n        if print_results_flag:\n            print(\"Results of Dickey-Fuller Test:\")\n            print(adf_output)\n\n        # Interpret results\n        if adf_output[\"p-value\"] &lt;= 0.05:\n            print(\"\\nData does not have a unit root. Data is STATIONARY!\")\n        else:\n            print(\"\\nData has a unit root. Data is NON-STATIONARY!\")\n\n    def check_NINF(self, data: Union[pd.DataFrame, np.ndarray]) -&gt; None:\n        \"\"\"\n        Check DataFrame or array for NaN and infinite values.\n\n        Args:\n            data: Input data to check\n\n        Usage:\n             tools = DSTools()\n             tools.check_NINF(data)\n        \"\"\"\n        if isinstance(data, pd.DataFrame):\n            has_nan = data.isnull().any().any()\n            has_inf = np.isinf(data.select_dtypes(include=[np.number])).any().any()\n        else:\n            has_nan = np.isnan(data).any()\n            has_inf = np.isinf(data).any()\n\n        if not has_nan and not has_inf:\n            print(\"Dataset has no NaN or infinite values\")\n        elif has_nan and not has_inf:\n            print(\"Dataset has NaN values but no infinite values\")\n        elif not has_nan and has_inf:\n            print(\"Dataset has infinite values but no NaN values\")\n        else:\n            print(\"Dataset has both NaN and infinite values\")\n\n    def df_stats(\n        self, df: pd.DataFrame, return_format: str = \"dict\", detailed: bool = True\n    ) -&gt; Union[dict, pd.DataFrame]:\n        \"\"\"\n        Provide quick overview of DataFrame structure.\n\n        Args:\n            df: Input DataFrame\n            return_format: Format of return ('dict' or 'dataframe')\n            detailed: Include additional statistics\n\n        Returns:\n            dict or DataFrame with statistics\n        \"\"\"\n        stats = {\n            \"columns\": df.shape[1],\n            \"rows\": df.shape[0],\n            \"missing_percent\": np.round(df.isnull().sum().sum() / df.size * 100, 1),\n            \"memory_mb\": np.round(df.memory_usage(deep=True).sum() / 10**6, 1),\n        }\n\n        if detailed:\n            stats.update(\n                {\n                    \"numeric_columns\": df.select_dtypes(include=[np.number]).shape[1],\n                    \"categorical_columns\": df.select_dtypes(\n                        include=[\"object\", \"category\"]\n                    ).shape[1],\n                    \"datetime_columns\": df.select_dtypes(include=[\"datetime\"]).shape[1],\n                    \"duplicated_rows\": df.duplicated().sum(),\n                    \"total_missing_values\": df.isnull().sum().sum(),\n                }\n            )\n\n        if return_format.lower() == \"dataframe\":\n            return pd.DataFrame(list(stats.items()), columns=[\"metric\", \"value\"])\n        else:\n            return stats\n\n    def describe_categorical(self, df: pd.DataFrame) -&gt; pd.DataFrame:\n        \"\"\"\n        Detailed description of categorical columns.\n\n        Args:\n            df: Input DataFrame\n\n        Returns:\n            DataFrame with categorical statistics\n\n        Usage:\n             tools = DSTools()\n             cat_stats = tools.describe_categorical(df)\n        \"\"\"\n        # 1. Select columns with types 'object', 'category', 'string'\n        categorical_cols = df.select_dtypes(\n            include=[\"object\", \"category\", \"string\"]\n        ).columns.tolist()\n\n        # 2. Find columns that ONLY consist of NaN (they can be of numeric type)\n        all_nan_cols = df.columns[df.isnull().all()].tolist()\n\n        # 3. Combine both lists and remove duplicates\n        cols_to_process = sorted(list(set(categorical_cols + all_nan_cols)))\n\n        if not cols_to_process:\n            return pd.DataFrame()\n\n        # 4. Get basic descriptive statistics\n        description = df[cols_to_process].describe(include=\"all\").T\n\n        # 5. Calculate the percentage of missing data\n        missing_percent = (df[cols_to_process].isnull().sum() / len(df) * 100).round(1)\n\n        # 6. Assemble the final DataFrame\n        result_df = description\n        result_df[\"missing (%)\"] = missing_percent\n\n        # 7. Order and clear the columns\n        # Remove 'count', as it duplicates the information about missing data\n        if \"count\" in result_df.columns:\n            result_df = result_df.drop(columns=\"count\")\n\n        final_cols_order = [\"missing (%)\", \"unique\", \"top\", \"freq\"]\n\n        # Leave only those columns from our ideal list that actually exist\n        existing_cols = [col for col in final_cols_order if col in result_df.columns]\n\n        return result_df[existing_cols]\n\n    def describe_numeric(self, df: pd.DataFrame) -&gt; pd.DataFrame:\n        \"\"\"\n        Detailed description of numerical columns.\n\n        Args:\n            df: Input DataFrame\n\n        Returns:\n            DataFrame with numerical statistics\n\n        Usage:\n             tools = DSTools()\n             num_stats = tools.describe_numeric(df)\n        \"\"\"\n        numeric_cols = df.select_dtypes(include=[np.number]).columns\n        if len(numeric_cols) == 0:\n            return pd.DataFrame()\n\n        numeric_df = df[numeric_cols]\n        description = numeric_df.describe()\n\n        result_data = {\n            \"sum\": numeric_df.sum(),\n            \"missing (%)\": np.round(numeric_df.isnull().sum() / len(df) * 100, 1),\n            \"median\": numeric_df.median(),\n            \"skew\": numeric_df.skew(),\n            \"kurtosis\": numeric_df.kurtosis(),\n        }\n\n        # Add description statistics\n        for stat in description.index:\n            if stat != \"count\":\n                result_data[stat] = description.loc[stat]\n\n        return pd.DataFrame(result_data, index=numeric_cols)\n\n    @staticmethod\n    def validate_moments(std: float, skewness: float, kurtosis: float) -&gt; bool:\n        \"\"\"\n        Validate that statistical moments are physically possible.\n        A key property is that kurtosis must be greater than or equal to\n        the square of skewness minus 2.\n\n        Args:\n            std: Standard deviation\n            skewness: Skewness value\n            kurtosis: Kurtosis value\n\n        Returns:\n            True if moments are valid, False otherwise\n\n        Usage:\n             tools = DSTools()\n             is_valid = tools.validate_moments(1.0, 0.5, 3.0)\n        \"\"\"\n        return std &gt; 0 and kurtosis &gt;= (skewness**2 - 2)\n\n    def generate_distribution(self, config: DistributionConfig) -&gt; np.ndarray:\n        \"\"\"\n        Generates a distribution matching the provided statistical metrics.\n\n        This function creates a distribution by generating a base dataset with a\n        shape defined by kurtosis, adds outliers, and then iteratively scales\n        and shifts the data to match the target mean and standard deviation\n        within a specified accuracy threshold.\n\n        Args:\n            config: A Pydantic model instance containing all configuration parameters.\n\n        Returns:\n            A NumPy array of numerical values with the specified properties.\n\n        Usage:\n             tools = DSTools()\n             config = DistributionConfig(\n            ...     mean=100, median=95, std=15, min_val=50, max_val=200,\n            ...     skewness=0.5, kurtosis=3.5, n=1000\n            ... )\n             data = tools.generate_distribution(config)\n             print(f'Generated Mean: {np.mean(data):.2f}, Std: {np.std(data):.2f}')\n        \"\"\"\n        if not self.validate_moments(config.std, config.skewness, config.kurtosis):\n            raise ValueError(\"Invalid statistical moments\")\n        if config.min_val &gt;= config.max_val:\n            raise ValueError(\"max_val must be greater than min_val\")\n\n        num_outliers = int(config.n * config.outlier_ratio)\n        num_base = config.n - num_outliers\n\n        # --- 1. Generate Base Distribution ---\n        # Generate a base distribution with a shape influenced by kurtosis.\n        # Student's t-distribution is used for heavy tails (kurtosis &gt; 3).\n        if config.kurtosis &gt; 3.5:\n            # Lower degrees of freedom lead to heavier tails\n            df = max(1, int(10 / (config.kurtosis - 2.5)))\n            base_data = stats.t.rvs(df=df, size=num_base)\n        else:\n            base_data = np.random.standard_normal(size=num_base)\n\n        # --- 2. Add Outliers ---\n        # Generate outliers to further influence the tails.\n        if num_outliers &gt; 0:\n            # Outliers are generated with a larger variance to be distinct.\n            outlier_scale = config.std * (1 + config.kurtosis / 3)\n            outliers = np.random.normal(loc=0, scale=outlier_scale, size=num_outliers)\n            data = np.concatenate([base_data, outliers])\n        else:\n            data = base_data\n\n        np.random.shuffle(data)\n\n        # --- 3. Iterative Scaling and Shifting ---\n        # Iteratively adjust the data to match the target mean and std.\n        # This is more stable than trying to adjust all moments at once.\n        max_iterations = 50\n\n        for _ in range(max_iterations):\n            current_mean = np.mean(data)\n            current_std = np.std(data, ddof=1)\n\n            # Check for convergence\n            mean_ok = abs(current_mean - config.mean) &lt; (\n                abs(config.mean) * config.accuracy_threshold\n            )\n            std_ok = abs(current_std - config.std) &lt; (\n                config.std * config.accuracy_threshold\n            )\n\n            if mean_ok and std_ok:\n                break\n\n            # Rescale and shift the data\n            if current_std &gt; EPSILON:\n                data = config.mean + (data - current_mean) * (config.std / current_std)\n            else:\n                # Handle case where all values are the same\n                data = np.full_like(data, config.mean)\n\n        # --- 4. Final Adjustments ---\n        # Clip data to ensure it's within the min/max bounds\n        data = np.clip(data, config.min_val, config.max_val)\n\n        # Ensure min and max values are present in the final distribution\n        # This can slightly alter the final moments but guarantees the range.\n        if data.min() &gt; config.min_val:\n            data[np.argmin(data)] = config.min_val\n        if data.max() &lt; config.max_val:\n            data[np.argmax(data)] = config.max_val\n\n        return data\n\n    @staticmethod\n    def evaluate_classification(\n        true_labels: np.ndarray,\n        pred_probs: np.ndarray,\n        threshold: float = 0.5,\n        figsize: Tuple[int, int] = (16, 7),\n    ) -&gt; Dict[str, Any]:\n        \"\"\"\n        Calculates, prints, and visualizes metrics for a binary classification model.\n\n        This \"all-in-one\" method provides a complete performance summary, including\n        key scalar metrics, a classification report, a confusion matrix, and\n        plots for ROC and Precision-Recall curves.\n\n        Args:\n            true_labels: Array of true binary labels (0 or 1).\n            pred_probs: Array of predicted probabilities for the positive class.\n            threshold: The cutoff to convert probabilities into binary predictions.\n            figsize: The size of the figure for the plots.\n\n        Returns:\n            A dictionary containing the calculated metrics for programmatic use.\n        \"\"\"\n        # --- 1. Input Validation ---\n        if not isinstance(true_labels, np.ndarray) or not isinstance(\n            pred_probs, np.ndarray\n        ):\n            raise TypeError(\"Inputs true_labels and pred_probs must be NumPy arrays.\")\n\n        if true_labels.shape != pred_probs.shape:\n            raise ValueError(\"Shape of true_labels and pred_probs must match.\")\n\n        # --- 2. Threshold-dependent Metrics ---\n        pred_labels = (pred_probs &gt;= threshold).astype(int)\n\n        accuracy = accuracy_score(true_labels, pred_labels)\n        report_dict = classification_report(\n            true_labels, pred_labels, output_dict=True, zero_division=0\n        )\n        conf_matrix = confusion_matrix(true_labels, pred_labels)\n\n        # --- 3. Threshold-independent Metrics ---\n        fpr, tpr, _ = roc_curve(true_labels, pred_probs)\n        ks = max(tpr - fpr)\n        roc_auc = auc(fpr, tpr)\n        avg_precision = average_precision_score(true_labels, pred_probs)\n        precision, recall, _ = precision_recall_curve(true_labels, pred_probs)\n\n        # --- 4. Console Output ---\n        print(\"*\" * 60)\n        print(\n            f\"{'CLASSIFICATION METRICS SUMMARY (Threshold = ' + str(threshold) + ')':^60}\"\n        )\n        print(\"*\" * 60)\n        print(f\"  - Accuracy          : {accuracy:.4f}\")\n        print(f\"  - ROC AUC           : {roc_auc:.4f}\")\n        print(f\"  - Average Precision : {avg_precision:.4f}\")\n        print(f\"  - Kolmogorov-Smirnov : {ks:.4f}\")\n        print(\"-\" * 60)\n\n        print(f\"\\n{'Classification Report':^60}\\n\")\n        report_df = pd.DataFrame(report_dict).transpose()\n        print(report_df.round(4))\n\n        print(f\"\\n{'Confusion Matrix':^60}\\n\")\n        print(\n            pd.DataFrame(\n                conf_matrix,\n                index=[\"Actual 0\", \"Actual 1\"],\n                columns=[\"Predicted 0\", \"Predicted 1\"],\n            )\n        )\n        print(\"*\" * 60)\n\n        # --- 5. Visualization ---\n        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=figsize)\n        plt.style.use(\"seaborn-v0_8-whitegrid\")\n\n        # Precision-Recall Curve\n        ax1.step(recall, precision, color=\"b\", alpha=0.8, where=\"post\")\n        ax1.fill_between(recall, precision, step=\"post\", alpha=0.2, color=\"b\")\n        ax1.set_xlabel(\"Recall\", fontsize=14)\n        ax1.set_ylabel(\"Precision\", fontsize=14)\n        ax1.set_ylim([0.0, 1.05])\n        ax1.set_xlim([0.0, 1.0])\n        ax1.set_title(f\"Precision-Recall Curve\\nAP = {avg_precision:.2f}\", fontsize=16)\n\n        # ROC Curve\n        ax2.plot(\n            fpr,\n            tpr,\n            color=\"darkorange\",\n            lw=2,\n            label=f\"ROC curve (AUC = {roc_auc:.2f}, KS = {ks:.2f})\",\n        )\n        ax2.plot([0, 1], [0, 1], color=\"navy\", lw=2, linestyle=\"--\")\n        ax2.fill_between(\n            fpr,\n            tpr,\n            fpr,\n            where=(tpr &gt;= fpr),\n            alpha=0.3,\n            color=\"green\",\n            interpolate=True,\n            label=\"Above random\",\n        )\n        ax2.set_xlim([0.0, 1.0])\n        ax2.set_ylim([0.0, 1.05])\n        ax2.set_xlabel(\"False Positive Rate\", fontsize=14)\n        ax2.set_ylabel(\"True Positive Rate\", fontsize=14)\n        ax2.set_title(\"Receiver Operating Characteristic (ROC)\", fontsize=16)\n        ax2.legend(loc=\"lower right\", fontsize=12)\n\n        plt.tight_layout()\n        plt.show()\n\n        # --- 6. Return Metrics Dictionary ---\n        return {\n            \"threshold\": threshold,\n            \"accuracy\": accuracy,\n            \"roc_auc\": roc_auc,\n            \"average_precision\": avg_precision,\n            \"Kolmogorov-Smirnov\": ks,\n            \"classification_report\": report_dict,\n            \"confusion_matrix\": conf_matrix,\n        }\n\n    @staticmethod\n    def grubbs_test(\n        x: Union[np.ndarray, pd.Series], alpha: float = 0.05\n    ) -&gt; GrubbsTestResult:\n        \"\"\"\n        Performs Grubbs' test to identify a single outlier in a dataset.\n\n        This test assumes the data comes from a normally distributed population\n        and is designed to detect one outlier at a time.\n\n        Args:\n            x: A 1D NumPy array or Pandas Series of numerical data.\n            alpha: The significance level for the test (default: 0.05).\n\n        Returns:\n            A Pydantic model (GrubbsTestResult) containing the test results,\n            including a boolean flag for outlier detection and the outlier's value\n            and index if found.\n\n        Raises:\n            ValueError: If the input array has fewer than 3 elements.\n\n        Usage:\n            tools = DSTools()\n\n            # Test 1: Data with an outlier\n            print(\"\\nTesting on data WITH an outlier:\")\n            result1 = tools.grubbs_test(data_with_outlier)\n            print(f\"  Calculated G-statistic: {result1.g_calculated:.4f}\")\n            print(f\"  Critical G-value: {result1.g_critical:.4f}\")\n            if result1.is_outlier:\n                print(f\"Outlier detected: The value is {result1.outlier_value:.2f} at index {result1.outlier_index}.\")\n            else:\n                print(\"No outlier detected.\")\n\n            # Test 2: Data without an outlier\n            print(\"\\nTesting on data WITHOUT an outlier:\")\n            result2 = tools.grubbs_test(data_without_outlier)\n            print(f\"  Calculated G-statistic: {result2.g_calculated:.4f}\")\n            print(f\"  Critical G-value: {result2.g_critical:.4f}\")\n            if result2.is_outlier:\n                print(f\"Outlier detected, but shouldn't have been.\")\n            else:\n                print(\"Correctly determined that there are no outliers.\")\n        \"\"\"\n        if not isinstance(x, (np.ndarray, pd.Series)):\n            raise TypeError(\"Input data x must be a NumPy array or Pandas Series.\")\n\n        # Grubbs' test requires at least 3 data points\n        n = len(x)\n        if n &lt; 3:\n            raise ValueError(\"Grubbs test requires at least 3 data points.\")\n\n        # Convert to numpy array for calculations\n        data = np.array(x)\n\n        # 1. Calculate the G-statistic\n        mean_x = np.mean(data)\n        std_x = np.std(data, ddof=1)  # Use sample standard deviation\n\n        if np.isclose(std_x, 0):\n            # If all values are the same, there are no outliers\n            return GrubbsTestResult(\n                is_outlier=False,\n                g_calculated=0.0,\n                g_critical=np.inf,  # Critical value is irrelevant here\n                outlier_value=None,\n                outlier_index=None,\n            )\n\n        max_deviation_index = np.argmax(np.abs(data - mean_x))\n        max_deviation_value = data[max_deviation_index]\n\n        numerator = np.abs(max_deviation_value - mean_x)\n        g_calculated = numerator / std_x\n\n        # 2. Calculate the critical G-value\n        t_value = stats.t.ppf(1 - alpha / (2 * n), n - 2)\n\n        numerator_critical = (n - 1) * t_value\n        denominator_critical = np.sqrt(n * (n - 2 + t_value**2))\n        g_critical = numerator_critical / denominator_critical\n\n        # 3. Compare and determine the result\n        is_outlier_detected = g_calculated &gt; g_critical\n\n        return GrubbsTestResult(\n            is_outlier=is_outlier_detected,\n            g_calculated=g_calculated,\n            g_critical=g_critical,\n            outlier_value=max_deviation_value if is_outlier_detected else None,\n            outlier_index=int(max_deviation_index) if is_outlier_detected else None,\n        )\n\n    @staticmethod\n    def plot_confusion_matrix(\n        y_true: Union[np.ndarray, pd.Series],\n        y_pred: Union[np.ndarray, pd.Series],\n        class_labels: Optional[List[str]] = None,\n        figsize: Tuple[int, int] = (8, 8),\n        title: str = \"Confusion Matrix\",\n        cmap: str = \"Blues\",\n    ):\n        \"\"\"\n        Plots a clear and readable confusion matrix using seaborn.\n\n        This method visualizes the performance of a classification model by showing\n        the number of correct and incorrect predictions for each class.\n\n        Args:\n            y_true: Array-like of true labels.\n            y_pred: Array-like of predicted labels.\n            class_labels: Optional list of strings to use as labels for the axes.\n                          If None, integer labels will be used.\n            figsize: Tuple specifying the figure size.\n            title: The title for the plot.\n            cmap: The colormap to use for the heatmap.\n        Usage:\n            tools = DSTools()\n\n            tools.plot_confusion_matrix(\n                y_true_binary,\n                y_pred_binary,\n                class_labels=['Negative (0)', 'Positive (1)'],\n                title='Binary Confusion Matrix'\n            )\n            tools.plot_confusion_matrix(\n                y_true_multi,\n                y_pred_multi,\n                class_labels=['Cat', 'Dog', 'Bird'],\n                title='Multi-Class Confusion Matrix'\n            )\n        \"\"\"\n        # 1. Calculate the confusion matrix\n        cm = confusion_matrix(y_true, y_pred)\n\n        # 2. Determine labels for the axes\n        if class_labels:\n            if len(class_labels) != cm.shape[0]:\n                raise ValueError(\n                    f\"Number of class_labels ({len(class_labels)}) does not match \"\n                    f\"number of classes in confusion matrix ({cm.shape[0]}).\"\n                )\n            labels = class_labels\n        else:\n            labels = np.arange(cm.shape[0])\n\n        # 3. Create the plot using seaborn's heatmap for better aesthetics\n        plt.style.use(\"seaborn-v0_8-whitegrid\")\n        fig, ax = plt.subplots(figsize=figsize)\n\n        sns.heatmap(\n            cm,\n            annot=True,  # Display the numbers in the cells\n            fmt=\"d\",  # Format numbers as integers\n            cmap=cmap,  # Use the specified colormap\n            xticklabels=labels,\n            yticklabels=labels,\n            ax=ax,  # Draw on our created axes\n            annot_kws={\"size\": 14},  # Increase annotation font size\n        )\n\n        # 4. Set titles and labels for clarity\n        ax.set_title(title, fontsize=16, pad=20)\n        ax.set_xlabel(\"Predicted Label\", fontsize=14)\n        ax.set_ylabel(\"True Label\", fontsize=14)\n\n        # Rotate tick labels for better readability if they are long\n        plt.xticks(rotation=45, ha=\"right\")\n        plt.yticks(rotation=0)\n\n        plt.tight_layout()\n        plt.show()\n\n    @staticmethod\n    def add_missing_value_features(\n        X: Union[pd.DataFrame, pl.DataFrame], add_std: bool = False\n    ) -&gt; Union[pd.DataFrame, pl.DataFrame]:\n        \"\"\"\n        Adds features based on the count of missing values per row.\n\n        This preprocessing function calculates the number of missing values (NaN)\n        for each row and adds this count as a new feature. This can significantly\n        improve the performance of some machine learning models.\n\n        Args:\n            X: The input DataFrame (Pandas or Polars).\n            add_std: If True, also adds the standard deviation of the nullity\n                     mask as a feature (rarely used).\n\n        Returns:\n            A new DataFrame with the added feature(s). The original DataFrame\n            is not modified.\n\n        Usage:\n            tools = DSTools()\n            pd_with_features = tools.add_missing_value_features(pd_data)\n            print(\"\\nPandas DataFrame with new feature:\")\n            print(pd_with_features)\n\n            pl_with_features = tools.add_missing_value_features(pl_data)\n            print(\"\\nPolars DataFrame with new feature:\")\n            print(pl_with_features)\n        \"\"\"\n        if isinstance(X, pd.DataFrame):\n            # --- Pandas Implementation ---\n            # Create a copy to avoid modifying the original DataFrame\n            X_new = X.copy()\n\n            # Calculate the number of missing values per row\n            num_missing = X.isnull().sum(axis=1)\n            X_new[\"num_missing\"] = num_missing\n\n            if add_std:\n                # Note: std of a boolean mask is often not very informative\n                num_missing_std = X.isnull().std(axis=1)\n                X_new[\"num_missing_std\"] = num_missing_std\n\n            return X_new\n\n        elif isinstance(X, pl.DataFrame):\n            # --- Polars Implementation (more efficient) ---\n            # Polars expressions are highly optimized\n            string_cols = [\n                col for col, dtype in X.schema.items() if dtype in [pl.String, pl.Utf8]\n            ]\n            numeric_cols = [col for col in X.columns if col not in string_cols]\n\n            expressions = []\n            if string_cols:\n                for col in string_cols:\n                    expr = (pl.col(col).is_null()) | (pl.col(col) == \"NaN\")\n                    expressions.append(expr)\n\n            if numeric_cols:\n                for col in numeric_cols:\n                    expr = pl.col(col).is_null()\n                    expressions.append(expr)\n\n            result_pl = X.with_columns(\n                pl.sum_horizontal(expressions).alias(\"num_missing\")\n            )\n\n            if add_std:\n                missing_matrix = X.select(expressions)\n                std_per_row = missing_matrix.select(\n                    pl.concat_list(pl.all())\n                    .list.eval(pl.element().cast(pl.Float64).std())\n                    .alias(\"num_missing_std\")\n                )\n                result_pl = result_pl.with_columns(std_per_row)\n\n            return result_pl\n\n        else:\n            raise TypeError(\"Input `X` must be a Pandas or Polars DataFrame.\")\n\n    @staticmethod\n    def chatterjee_correlation(\n        x: Union[np.ndarray, pd.Series, List[float]],\n        y: Union[np.ndarray, pd.Series, List[float]],\n        standard_flag: bool = True,\n    ) -&gt; float:\n        \"\"\"\n        Calculates Chatterjee's rank correlation coefficient (Xi).\n\n        This coefficient is a non-parametric measure of dependence between two\n        variables. It is asymmetric and ranges from 0 to 1, where a value\n        close to 1 indicates that y is a function of x. It can capture\n        non-linear relationships.\n\n        Args:\n            x: Array-like, the first variable (independent).\n            y: Array-like, the second variable (dependent).\n            standard_flag: bool flag which define type of calculation\n\n        Returns:\n            The Chatterjee's correlation coefficient, a float between 0 and 1.\n\n        Raises:\n            ValueError: If the input arrays do not have the same length.\n\n        Usage:\n             x = np.linspace(0, 10, 100)\n             y_linear = 2 * x + 1\n             y_nonlinear = np.sin(x)\n             tools = DSTools()\n             print(f\"Linear correlation: {tools.chatterjee_correlation(x, y_linear):.4f}\")\n             print(f\"Non-linear correlation: {tools.chatterjee_correlation(x, y_nonlinear):.4f}\")\n        \"\"\"\n        # 1. Convert inputs to NumPy arrays and validate\n        x_arr = np.asarray(x)\n        y_arr = np.asarray(y)\n\n        n = len(x_arr)\n        if n != len(y_arr):\n            raise ValueError(\"Input arrays x and y must have the same length.\")\n\n        if n &lt; 2:\n            return 0.0  # Correlation is undefined for less than 2 points\n\n        # 2. Get the ranks of y based on the sorted order of x\n        # argsort gives the indices that would sort x\n        x_order_indices = np.argsort(x_arr)\n\n        # Reorder y according to the sorted x\n        y_ordered_by_x = y_arr[x_order_indices]\n\n        # Calculate ranks of the reordered y. 'average' method handles ties.\n        # This replaces the dependency on pandas.Series.rank()\n        y_ranks = rankdata(y_ordered_by_x, method=\"average\")\n\n        # 3. Calculate the sum of absolute differences of consecutive ranks\n        # np.diff calculates the difference between adjacent elements\n        rank_diffs_sum = np.sum(np.abs(np.diff(y_ranks)))\n\n        # 4. Calculate Chatterjee's Xi coefficient\n        # The original formula is 1 - (3 * sum(|r_{i+1} - r_i|)) / (n^2 - 1)\n        # An equivalent and more stable formula is used below.\n        xi_orig = 1 - (n * rank_diffs_sum) / (\n            2 * np.sum(np.abs(y_ranks - np.mean(y_ranks)) ** 2)\n        )\n\n        xi = 1 - (3 * rank_diffs_sum) / (n**2 - 1) if standard_flag else xi_orig\n\n        return xi\n\n    @staticmethod\n    def calculate_entropy(\n        p: Union[np.ndarray, List[float]], base: Optional[float] = None\n    ) -&gt; float:\n        \"\"\"\n        Calculates the Shannon entropy of a probability distribution.\n\n        Entropy measures the uncertainty or \"surprise\" inherent in a variable's\n        possible outcomes.\n\n        Args:\n            p: A 1D array-like object representing a probability distribution.\n               The sum of its elements should be close to 1.\n            base: The logarithmic base to use for the calculation. If None (default),\n                  the natural logarithm (ln) is used, and the result is in \"nats\".\n                  Use base=2 for the result in \"bits\".\n\n        Returns:\n            The calculated entropy as a float.\n\n        Raises:\n            ValueError: If the input array contains negative values.\n\n        Usage:\n            tools = DSTools()\n            print(\"\\nCalculating Entropy (in nats):\")\n            entropy_a = tools.calculate_entropy(dist_a)\n            entropy_uniform = tools.calculate_entropy(dist_uniform)\n            print(f\"  - Entropy of A [0.1, 0.2, 0.7]: {entropy_a:.4f}\")\n            print(f\"  - Entropy of Uniform [0.33, 0.33, 0.33]: {entropy_uniform:.4f} (should be highest)\")\n\n            entropy_a_bits = tools.calculate_entropy(dist_a, base=2)\n            print(f\"  - Entropy of A in bits: {entropy_a_bits:.4f}\")\n        \"\"\"\n        p_arr = np.asarray(p, dtype=float)\n\n        if np.any(p_arr &lt; 0):\n            raise ValueError(\"Probabilities cannot be negative.\")\n\n        # Normalize the distribution to ensure it sums to 1\n        p_arr /= np.sum(p_arr)\n\n        # Filter out zero probabilities to avoid issues with log(0)\n        p_arr = p_arr[p_arr &gt; 0]\n\n        if base is None:\n            # Use natural logarithm (nats)\n            return -np.sum(p_arr * np.log(p_arr))\n        else:\n            # Use specified base\n            return -np.sum(p_arr * np.log(p_arr) / np.log(base))\n\n    @staticmethod\n    def calculate_kl_divergence(\n        p: Union[np.ndarray, List[float]],\n        q: Union[np.ndarray, List[float]],\n        base: Optional[float] = None,\n    ) -&gt; float:\n        \"\"\"\n        Calculates the Kullback-Leibler (KL) divergence between two distributions.\n\n        KL divergence D_KL(P || Q) measures how one probability distribution P\n        diverges from a second, expected probability distribution Q. It is\n        asymmetric.\n\n        Args:\n            p: A 1D array-like object for the \"true\" or reference distribution (P).\n            q: A 1D array-like object for the \"approximating\" distribution (Q).\n            base: The logarithmic base to use. Defaults to natural log (nats).\n\n        Returns:\n            The KL divergence as a float.\n\n        Raises:\n            ValueError: If input arrays have different lengths or contain negative values.\n\n        Usage:\n            print(\"\\nCalculating KL Divergence (D_KL(P || Q)):\")\n            # Divergence of a distribution from itself should be 0\n            kl_a_a = tools.calculate_kl_divergence(dist_a, dist_a)\n            print(f\"  - KL(A || A): {kl_a_a:.4f} (should be 0)\")\n\n            # Divergence of A from B (B is a good approximation of A)\n            kl_a_b = tools.calculate_kl_divergence(dist_a, dist_b)\n            print(f\"  - KL(A || B): {kl_a_b:.4f} (should be small)\")\n\n            # Divergence of A from C (C is a bad approximation of A)\n            kl_a_c = tools.calculate_kl_divergence(dist_a, dist_c)\n            print(f\"  - KL(A || C): {kl_a_c:.4f} (should be large)\")\n\n            # Note that KL divergence is asymmetric\n            kl_c_a = tools.calculate_kl_divergence(dist_c, dist_a)\n            print(f\"  - KL(C || A): {kl_c_a:.4f} (note: not equal to KL(A || C))\")\n        \"\"\"\n        p_arr = np.asarray(p, dtype=float)\n        q_arr = np.asarray(q, dtype=float)\n\n        if p_arr.shape != q_arr.shape:\n            raise ValueError(\"Input distributions P and Q must have the same shape.\")\n\n        if np.any(p_arr &lt; 0) or np.any(q_arr &lt; 0):\n            raise ValueError(\"Probabilities cannot be negative.\")\n\n        # Normalize both distributions\n        p_arr /= np.sum(p_arr)\n        q_arr /= np.sum(q_arr)\n\n        # Add a small epsilon to avoid division by zero or log(0)\n        # We only need to protect q from being zero where p is non-zero\n        p_arr += EPSILON\n        q_arr += EPSILON\n\n        if base is None:\n            return np.sum(p_arr * np.log(p_arr / q_arr))\n        else:\n            return np.sum(p_arr * (np.log(p_arr / q_arr) / np.log(base)))\n\n    @staticmethod\n    def min_max_scale(\n        df: Union[pd.DataFrame, pl.DataFrame],\n        columns: Optional[List[str]] = None,\n        const_val_fill: float = 0.0,\n    ) -&gt; Union[pd.DataFrame, pl.DataFrame]:\n        \"\"\"\n        Scales specified columns of a DataFrame to the range [0, 1].\n\n        This method applies Min-Max scaling. If a column contains identical\n        values, it will be filled with `const_val_fill`. The original\n        DataFrame is not modified.\n\n        Args:\n            df: The input DataFrame (Pandas or Polars).\n            columns: A list of column names to scale. If None (default),\n                     all numerical columns will be scaled.\n            const_val_fill: The value to use for columns where all values\n                            are identical (to avoid division by zero).\n\n        Returns:\n            A new DataFrame with the specified columns scaled.\n\n        Usage:\n            tools = DSTools()\n            pd_scaled = tools.min_max_scale(pd_data, columns=['a', 'c'])\n            print(\"\\nPandas DataFrame with scaled columns 'a' and 'c':\")\n            print(pd_scaled)\n\n            pl_scaled = tools.min_max_scale(pl_data) # Scale all numeric columns\n            print(\"\\nPolars DataFrame with all numeric columns scaled:\")\n            print(pl_scaled)\n\n            pl_scaled_half = tools.min_max_scale(pl_data, const_val_fill=0.5)\n            print(\"\\nPolars DataFrame with constant columns filled with 0.5:\")\n            print(pl_scaled_half)\n        \"\"\"\n        if isinstance(df, pd.DataFrame):\n            # --- Pandas Implementation ---\n            df_new = df.copy()\n\n            if columns is None:\n                # Select all numeric columns if none are specified\n                columns = df_new.select_dtypes(include=np.number).columns.tolist()\n\n            for col in columns:\n                if col not in df_new.columns:\n                    print(f\"Warning: Column '{col}' not found in DataFrame. Skipping.\")\n                    continue\n\n                min_val = df_new[col].min()\n                max_val = df_new[col].max()\n\n                if min_val == max_val:\n                    df_new[col] = const_val_fill\n                else:\n                    df_new[col] = (df_new[col] - min_val) / (max_val - min_val)\n\n            return df_new\n\n        elif isinstance(df, pl.DataFrame):\n            # --- Polars Implementation ---\n\n            if columns is None:\n                # Select all numeric columns\n                columns = [\n                    col for col, dtype in df.schema.items() if dtype.is_numeric()\n                ]\n\n            expressions = []\n            for col in columns:\n                if col not in df.columns:\n                    print(f\"Warning: Column '{col}' not found in DataFrame. Skipping.\")\n                    continue\n\n                min_val = pl.col(col).min()\n                max_val = pl.col(col).max()\n\n                # Use a 'when/then/otherwise' expression for conditional logic\n                expr = (\n                    pl.when(min_val == max_val)\n                    .then(pl.lit(const_val_fill))\n                    .otherwise((pl.col(col) - min_val) / (max_val - min_val))\n                    .alias(col)  # Keep the original column name\n                )\n                expressions.append(expr)\n\n            return df.with_columns(expressions)\n\n        else:\n            raise TypeError(\"Input dataframe must be a Pandas or Polars DataFrame.\")\n\n    @staticmethod\n    def save_dataframes_to_zip(\n        dataframes: Dict[str, Union[pd.DataFrame, pl.DataFrame]],\n        zip_filename: str,\n        format: str = \"parquet\",\n        save_index: bool = False,\n    ):\n        \"\"\"\n        Saves one or more Pandas or Polars DataFrames into a single ZIP archive.\n\n        Args:\n            dataframes: A dictionary where keys are the desired filenames (without\n                        extension) and values are the Pandas or Polars DataFrames.\n            zip_filename: The path for the output ZIP archive.\n            format: The format to save the data in ('parquet', 'csv').\n            save_index: For Pandas DataFrames, whether to save the index.\n                        (Ignored for Polars).\n\n        Usage:\n            tools = DSTools()\n            dfs_to_save = {\n                'pandas_data': pd_df,\n                'polars_data': pl_df\n            }\n            zip_path = 'mixed_data_archive.zip'\n            print(\"\\n--- Saving mixed DataFrames ---\")\n            tools.save_dataframes_to_zip(dfs_to_save, zip_path, format='parquet', save_index=True)\n\n            print(\"\\n--- Reading back with Polars backend ---\")\n            loaded_with_polars = tools.read_dataframes_from_zip(zip_path, format='parquet', backend='polars')\n            print(\"DataFrame 'pandas_data' loaded by Polars:\")\n            print(loaded_with_polars['pandas_data']) # The index will be lost because Polars does not have it.\n\n            print(\"\\n--- Reading back with Pandas backend ---\")\n            loaded_with_pandas = tools.read_dataframes_from_zip(zip_path, format='parquet', backend='pandas')\n            print(\"DataFrame 'pandas_data' loaded by Pandas:\")\n            print(loaded_with_pandas['pandas_data']) # The index will be restored\n\n            if os.path.exists(zip_path):\n            os.remove(zip_path)\n        \"\"\"\n        if not isinstance(dataframes, dict):\n            raise TypeError(\"`dataframes` must be a dictionary of {filename: df}.\")\n\n        with tempfile.TemporaryDirectory() as temp_dir:\n            file_paths = []\n\n            for name, df in dataframes.items():\n                safe_name = re.sub(r'[\\\\/*?:\"&lt;&gt;|]', \"_\", name)\n                file_path = os.path.join(temp_dir, f\"{safe_name}.{format}\")\n                file_paths.append(file_path)\n\n                # --- Dispatch based on DataFrame type ---\n                if isinstance(df, pd.DataFrame):\n                    if format == \"parquet\":\n                        df.to_parquet(file_path, index=save_index, engine=\"fastparquet\")\n                    elif format == \"csv\":\n                        df.to_csv(file_path, index=save_index)\n                    else:\n                        raise ValueError(f\"Unsupported format: '{format}'.\")\n                elif isinstance(df, pl.DataFrame):\n                    if format == \"parquet\":\n                        df.write_parquet(file_path)\n                    elif format == \"csv\":\n                        df.write_csv(file_path)\n                    else:\n                        raise ValueError(f\"Unsupported format: '{format}'.\")\n                else:\n                    raise TypeError(f\"Unsupported DataFrame type: {type(df)}\")\n\n            # Create the ZIP archive\n            with zipfile.ZipFile(zip_filename, \"w\", zipfile.ZIP_DEFLATED) as zipf:\n                for path in file_paths:\n                    zipf.write(path, os.path.basename(path))\n\n        print(f\"Successfully saved {len(dataframes)} DataFrame(s) to {zip_filename}\")\n\n    @staticmethod\n    def read_dataframes_from_zip(\n        zip_filename: str, format: str = \"parquet\", backend: str = \"polars\"\n    ) -&gt; Dict[str, Union[pd.DataFrame, pl.DataFrame]]:\n        \"\"\"\n        Reads one or more DataFrames from a ZIP archive.\n\n        Args:\n            zip_filename: The path to the ZIP archive.\n            format: The format of the files inside the ZIP ('parquet', 'csv').\n            backend: The library to use for reading ('polars' or 'pandas').\n\n        Returns:\n            A dictionary where keys are the filenames (without extension) and\n            values are the loaded DataFrames.\n        \"\"\"\n        if backend not in [\"polars\", \"pandas\"]:\n            raise ValueError(\"`backend` must be 'polars' or 'pandas'.\")\n\n        loaded_dataframes = {}\n\n        with tempfile.TemporaryDirectory() as temp_dir:\n            with zipfile.ZipFile(zip_filename, \"r\") as zipf:\n                zipf.extractall(temp_dir)\n\n            extension = f\".{format}\"\n            for filename in os.listdir(temp_dir):\n                if filename.endswith(extension):\n                    file_path = os.path.join(temp_dir, filename)\n                    name_without_ext = os.path.splitext(filename)[0]\n\n                    # --- Dispatch based on a chosen backend ---\n                    if backend == \"polars\":\n                        df = (\n                            pl.read_parquet(file_path)\n                            if format == \"parquet\"\n                            else pl.read_csv(file_path)\n                        )\n                    else:  # backend == 'pandas'\n                        if format == \"parquet\":\n                            df = pd.read_parquet(file_path)\n                        else:\n                            try:\n                                df = pd.read_csv(file_path, index_col=0)\n                            except (ValueError, IndexError):\n                                df = pd.read_csv(file_path)\n\n                    loaded_dataframes[name_without_ext] = df\n\n        print(\n            f\"Successfully loaded {len(loaded_dataframes)} DataFrame(s) using {backend}.\"\n        )\n        return loaded_dataframes\n\n    @staticmethod\n    def generate_alphanum_codes(n: int, length: int = 8) -&gt; np.ndarray:\n        \"\"\"\n        Generates an array of random alphanumeric codes.\n\n        This method is optimized for performance by using NumPy vectorized operations.\n\n        Args:\n            n: The number of codes to generate.\n            length: The length of each code.\n\n        Returns:\n            A NumPy array of strings, where each string is a random code.\n\n        Usage:\n            tools = DSTools()\n            codes = tools.generate_alphanum_codes(5, length=10)\n            print(f\"Generated codes:\\n{codes}\")\n        \"\"\"\n        if n &lt; 0 or length &lt; 0:\n            raise ValueError(\"Number of codes (n) and length must be non-negative.\")\n\n        if length == 0:\n            return np.full(n, \"\", dtype=str)\n\n        # A clean, non-repeating alphabet\n        alphabet = \"0123456789ABCDEFGHIJKLMNOPQRSTUVWXYZ\"\n        alphabet_len = len(alphabet)\n\n        # Generate all random indices at once\n        random_indices = np.random.randint(0, alphabet_len, size=(n, length))\n\n        # Use NumPy's advanced indexing to get characters\n        # .view('S1') treats each character as a 1-byte string\n        # .reshape converts back to the desired shape\n        codes_as_chars = np.array(list(alphabet), dtype=\"S1\")[random_indices]\n\n        # .view('S{length}') joins the characters in each row into a single string\n        # This is a highly optimized, low-level NumPy operation\n        codes_as_bytes = codes_as_chars.view(f\"S{length}\")\n\n        # Decode from bytes to a standard UTF-8 string array\n        return np.char.decode(codes_as_bytes.flatten(), \"utf-8\")\n\n    def generate_distribution_from_metrics(\n        self,\n        n: int,\n        metrics: Union[DistributionConfig, Dict[str, float]],\n        int_flag: bool = True,\n        output_as: Literal[\"numpy\", \"pandas\", \"polars\"] = \"numpy\",\n        max_iterations: int = 100,\n    ) -&gt; Union[np.ndarray, pd.Series, pl.Series]:\n        \"\"\"\n        Generates a synthetic distribution matching given statistical metrics.\n\n        This function uses an iterative approach to create a distribution that\n        approximates the properties specified in the DistributionConfig.\n\n        Args:\n            n: Number of values to generate.\n            metrics: A Pydantic `DistributionConfig` instance OR a dictionary\n                     with the target statistical properties.\n            int_flag: If True, returns integer values; otherwise, floats.\n            output_as: The desired output format ('numpy', 'pandas', or 'polars').\n            max_iterations: int value is a number of iterations for tries.\n\n        Returns:\n            An array or Series of generated values.\n\n        Usage:\n            tools = DSTools()\n            try:\n                metrics_dict = DistributionConfig(\n                    mean=1042,\n                    median=330,\n                    std=1500,\n                    min_val=1,\n                    max_val=120000,\n                    skewness=13.2,\n                    kurtosis=245,\n                    n=10000,\n                    accuracy_threshold=0.05,\n                    outlier_ratio=0.05\n                )\n\n                # 2. Generate the data using the metrics object\n                generated_data = tools.generate_distribution_from_metrics(\n                    n=1000,\n                    metrics=metrics_dict,\n                    int_flag=True,\n                    output_as='numpy'\n                )\n\n                # 3. Analyze the result\n                print(\"--- Target vs. Actual Statistics ---\")\n                print(f\"Target Mean: {metrics.mean}, Actual Mean: {np.mean(generated_data):.2f}\")\n                print(f\"Target Median: {metrics.median}, Actual Median: {np.median(generated_data):.2f}\")\n                print(f\"Target Std: {metrics.std}, Actual Std: {np.std(generated_data):.2f}\")\n                print(f\"Target Skew: {metrics.skewness}, Actual Skew: {stats.skew(generated_data):.2f}\")\n                print(f\"Target Kurtosis: {metrics.kurtosis}, Actual Kurtosis: {stats.kurtosis(generated_data, fisher=False):.2f}\")\n                print(f\"Target Min: {metrics.min_val}, Actual Min: {np.min(generated_data):.2f}\")\n                print(f\"Target Max: {metrics.max_val}, Actual Max: {np.max(generated_data):.2f}\")\n\n            except ValueError as e:\n                print(f\"Error during configuration or generation: {e}\")\n        \"\"\"\n        if isinstance(metrics, dict):\n            try:\n                config = DistributionConfig(**metrics)\n            except Exception as e:\n                raise ValueError(f\"Invalid metrics dictionary: {e}\")\n        elif isinstance(metrics, DistributionConfig):\n            config = metrics\n        else:\n            raise TypeError(\"Invalid metrics dictionary\")\n\n        if config.n is None:\n            config.n = n\n        elif config.n != n:\n            print(\n                f\"Warning: `n` provided in both arguments ({n}) and config ({config.n}). \"\n                f\"Using value from arguments: {n}.\"\n            )\n            config.n = n\n\n        if not self.validate_moments(config.std, config.skewness, config.kurtosis):\n            raise ValueError(\"Invalid metrics dictionary\")\n\n        # --- 1. Initial Data Generation ---\n        num_outliers = int(config.n * config.outlier_ratio)\n        num_base = config.n - num_outliers\n\n        # Generate the main part of the distribution\n        # Use a non-central t-distribution to introduce initial skew\n        nc = config.skewness * (config.kurtosis / 3.0)  # Heuristic for non-centrality\n        df = max(\n            5, int(6 + 2 * (config.kurtosis - 3))\n        )  # Degrees of freedom influence kurtosis\n\n        base_data = stats.nct.rvs(df=df, nc=nc, size=num_base)\n\n        # Generate outliers to control the tails\n        if num_outliers &gt; 0:\n            # Generate outliers from a wider normal distribution\n            outlier_scale = config.std * (1.5 + config.kurtosis / 5.0)\n            outliers = np.random.normal(\n                loc=config.mean, scale=outlier_scale, size=num_outliers\n            )\n            data = np.concatenate([base_data, outliers])\n        else:\n            data = base_data\n\n        np.random.shuffle(data)\n\n        # Initial scaling to get closer to the target\n        data = config.mean + (data - np.mean(data)) * (\n            config.std / (np.std(data) + EPSILON)\n        )\n\n        # --- 2. Iterative Adjustment ---\n        for _ in range(max_iterations):\n            # Calculate current moments\n            current_mean = np.mean(data)\n            current_std = np.std(data, ddof=1)\n            current_median = np.median(data)\n\n            # Check for convergence on primary metrics (mean, std, median)\n            mean_ok = abs(current_mean - config.mean) &lt; (\n                abs(config.mean) * config.accuracy_threshold\n            )\n            std_ok = abs(current_std - config.std) &lt; (\n                config.std * config.accuracy_threshold\n            )\n            median_ok = abs(current_median - config.median) &lt; (\n                abs(config.median) * config.accuracy_threshold\n            )\n\n            if mean_ok and std_ok and median_ok:\n                break\n\n            # Adjustment for mean and std (rescale and shift)\n            if current_std &gt; EPSILON:\n                data = config.mean + (data - current_mean) * (config.std / current_std)\n\n            # Adjustment for median (gentle push towards the target)\n            median_diff = config.median - np.median(data)\n            # Apply a non-linear shift to move the median without ruining the mean/std too much\n            data += (\n                median_diff\n                * np.exp(-(((data - np.median(data)) / config.std) ** 2))\n                * 0.1\n            )\n\n        # --- 3. Finalization ---\n        # Final clipping and type casting\n        data = np.clip(data, config.min_val, config.max_val)\n\n        # Ensure min/max values are present\n        if data.min() &gt; config.min_val:\n            data[np.argmin(data)] = config.min_val\n\n        if data.max() &lt; config.max_val:\n            data[np.argmax(data)] = config.max_val\n\n        if int_flag:\n            data = np.round(data).astype(np.int64)\n\n        # Convert to desired output format\n        if output_as == \"pandas\":\n            return pd.Series(data, name=\"generated_values\")\n        elif output_as == \"polars\":\n            return pl.Series(name=\"generated_values\", values=data)\n        else:\n            return data\n</code></pre>"},{"location":"api/ds_tool/#ds_tool.DSTools.__init__","title":"<code>__init__()</code>","text":"<p>Initialize the DSTools class with default configurations.</p> Source code in <code>src\\ds_tool.py</code> <pre><code>def __init__(self):\n    \"\"\"Initialize the DSTools class with default configurations.\"\"\"\n\n    plt.rcParams[\"figure.figsize\"] = (15, 9)\n    pd.options.display.float_format = \"{:.2f}\".format\n    np.set_printoptions(suppress=True, precision=4)\n\n    # Set random seeds for reproducibility\n    random_seed = 42\n    np.random.seed(random_seed)\n    random.seed(random_seed)\n\n    # Theme configuration\n    self.plotly_theme = \"plotly_dark\"\n</code></pre>"},{"location":"api/ds_tool/#ds_tool.DSTools.add_missing_value_features","title":"<code>add_missing_value_features(X, add_std=False)</code>  <code>staticmethod</code>","text":"<pre><code>    Adds features based on the count of missing values per row.\n\n    This preprocessing function calculates the number of missing values (NaN)\n    for each row and adds this count as a new feature. This can significantly\n    improve the performance of some machine learning models.\n\n    Args:\n        X: The input DataFrame (Pandas or Polars).\n        add_std: If True, also adds the standard deviation of the nullity\n                 mask as a feature (rarely used).\n\n    Returns:\n        A new DataFrame with the added feature(s). The original DataFrame\n        is not modified.\n\n    Usage:\n        tools = DSTools()\n        pd_with_features = tools.add_missing_value_features(pd_data)\n        print(\"\n</code></pre> <p>Pandas DataFrame with new feature:\")             print(pd_with_features)</p> <pre><code>        pl_with_features = tools.add_missing_value_features(pl_data)\n        print(\"\n</code></pre> <p>Polars DataFrame with new feature:\")             print(pl_with_features)</p> Source code in <code>src\\ds_tool.py</code> <pre><code>@staticmethod\ndef add_missing_value_features(\n    X: Union[pd.DataFrame, pl.DataFrame], add_std: bool = False\n) -&gt; Union[pd.DataFrame, pl.DataFrame]:\n    \"\"\"\n    Adds features based on the count of missing values per row.\n\n    This preprocessing function calculates the number of missing values (NaN)\n    for each row and adds this count as a new feature. This can significantly\n    improve the performance of some machine learning models.\n\n    Args:\n        X: The input DataFrame (Pandas or Polars).\n        add_std: If True, also adds the standard deviation of the nullity\n                 mask as a feature (rarely used).\n\n    Returns:\n        A new DataFrame with the added feature(s). The original DataFrame\n        is not modified.\n\n    Usage:\n        tools = DSTools()\n        pd_with_features = tools.add_missing_value_features(pd_data)\n        print(\"\\nPandas DataFrame with new feature:\")\n        print(pd_with_features)\n\n        pl_with_features = tools.add_missing_value_features(pl_data)\n        print(\"\\nPolars DataFrame with new feature:\")\n        print(pl_with_features)\n    \"\"\"\n    if isinstance(X, pd.DataFrame):\n        # --- Pandas Implementation ---\n        # Create a copy to avoid modifying the original DataFrame\n        X_new = X.copy()\n\n        # Calculate the number of missing values per row\n        num_missing = X.isnull().sum(axis=1)\n        X_new[\"num_missing\"] = num_missing\n\n        if add_std:\n            # Note: std of a boolean mask is often not very informative\n            num_missing_std = X.isnull().std(axis=1)\n            X_new[\"num_missing_std\"] = num_missing_std\n\n        return X_new\n\n    elif isinstance(X, pl.DataFrame):\n        # --- Polars Implementation (more efficient) ---\n        # Polars expressions are highly optimized\n        string_cols = [\n            col for col, dtype in X.schema.items() if dtype in [pl.String, pl.Utf8]\n        ]\n        numeric_cols = [col for col in X.columns if col not in string_cols]\n\n        expressions = []\n        if string_cols:\n            for col in string_cols:\n                expr = (pl.col(col).is_null()) | (pl.col(col) == \"NaN\")\n                expressions.append(expr)\n\n        if numeric_cols:\n            for col in numeric_cols:\n                expr = pl.col(col).is_null()\n                expressions.append(expr)\n\n        result_pl = X.with_columns(\n            pl.sum_horizontal(expressions).alias(\"num_missing\")\n        )\n\n        if add_std:\n            missing_matrix = X.select(expressions)\n            std_per_row = missing_matrix.select(\n                pl.concat_list(pl.all())\n                .list.eval(pl.element().cast(pl.Float64).std())\n                .alias(\"num_missing_std\")\n            )\n            result_pl = result_pl.with_columns(std_per_row)\n\n        return result_pl\n\n    else:\n        raise TypeError(\"Input `X` must be a Pandas or Polars DataFrame.\")\n</code></pre>"},{"location":"api/ds_tool/#ds_tool.DSTools.calculate_entropy","title":"<code>calculate_entropy(p, base=None)</code>  <code>staticmethod</code>","text":"<pre><code>    Calculates the Shannon entropy of a probability distribution.\n\n    Entropy measures the uncertainty or \"surprise\" inherent in a variable's\n    possible outcomes.\n\n    Args:\n        p: A 1D array-like object representing a probability distribution.\n           The sum of its elements should be close to 1.\n        base: The logarithmic base to use for the calculation. If None (default),\n              the natural logarithm (ln) is used, and the result is in \"nats\".\n              Use base=2 for the result in \"bits\".\n\n    Returns:\n        The calculated entropy as a float.\n\n    Raises:\n        ValueError: If the input array contains negative values.\n\n    Usage:\n        tools = DSTools()\n        print(\"\n</code></pre> <p>Calculating Entropy (in nats):\")             entropy_a = tools.calculate_entropy(dist_a)             entropy_uniform = tools.calculate_entropy(dist_uniform)             print(f\"  - Entropy of A [0.1, 0.2, 0.7]: {entropy_a:.4f}\")             print(f\"  - Entropy of Uniform [0.33, 0.33, 0.33]: {entropy_uniform:.4f} (should be highest)\")</p> <pre><code>        entropy_a_bits = tools.calculate_entropy(dist_a, base=2)\n        print(f\"  - Entropy of A in bits: {entropy_a_bits:.4f}\")\n</code></pre> Source code in <code>src\\ds_tool.py</code> <pre><code>@staticmethod\ndef calculate_entropy(\n    p: Union[np.ndarray, List[float]], base: Optional[float] = None\n) -&gt; float:\n    \"\"\"\n    Calculates the Shannon entropy of a probability distribution.\n\n    Entropy measures the uncertainty or \"surprise\" inherent in a variable's\n    possible outcomes.\n\n    Args:\n        p: A 1D array-like object representing a probability distribution.\n           The sum of its elements should be close to 1.\n        base: The logarithmic base to use for the calculation. If None (default),\n              the natural logarithm (ln) is used, and the result is in \"nats\".\n              Use base=2 for the result in \"bits\".\n\n    Returns:\n        The calculated entropy as a float.\n\n    Raises:\n        ValueError: If the input array contains negative values.\n\n    Usage:\n        tools = DSTools()\n        print(\"\\nCalculating Entropy (in nats):\")\n        entropy_a = tools.calculate_entropy(dist_a)\n        entropy_uniform = tools.calculate_entropy(dist_uniform)\n        print(f\"  - Entropy of A [0.1, 0.2, 0.7]: {entropy_a:.4f}\")\n        print(f\"  - Entropy of Uniform [0.33, 0.33, 0.33]: {entropy_uniform:.4f} (should be highest)\")\n\n        entropy_a_bits = tools.calculate_entropy(dist_a, base=2)\n        print(f\"  - Entropy of A in bits: {entropy_a_bits:.4f}\")\n    \"\"\"\n    p_arr = np.asarray(p, dtype=float)\n\n    if np.any(p_arr &lt; 0):\n        raise ValueError(\"Probabilities cannot be negative.\")\n\n    # Normalize the distribution to ensure it sums to 1\n    p_arr /= np.sum(p_arr)\n\n    # Filter out zero probabilities to avoid issues with log(0)\n    p_arr = p_arr[p_arr &gt; 0]\n\n    if base is None:\n        # Use natural logarithm (nats)\n        return -np.sum(p_arr * np.log(p_arr))\n    else:\n        # Use specified base\n        return -np.sum(p_arr * np.log(p_arr) / np.log(base))\n</code></pre>"},{"location":"api/ds_tool/#ds_tool.DSTools.calculate_kl_divergence","title":"<code>calculate_kl_divergence(p, q, base=None)</code>  <code>staticmethod</code>","text":"<pre><code>    Calculates the Kullback-Leibler (KL) divergence between two distributions.\n\n    KL divergence D_KL(P || Q) measures how one probability distribution P\n    diverges from a second, expected probability distribution Q. It is\n    asymmetric.\n\n    Args:\n        p: A 1D array-like object for the \"true\" or reference distribution (P).\n        q: A 1D array-like object for the \"approximating\" distribution (Q).\n        base: The logarithmic base to use. Defaults to natural log (nats).\n\n    Returns:\n        The KL divergence as a float.\n\n    Raises:\n        ValueError: If input arrays have different lengths or contain negative values.\n\n    Usage:\n        print(\"\n</code></pre> <p>Calculating KL Divergence (D_KL(P || Q)):\")             # Divergence of a distribution from itself should be 0             kl_a_a = tools.calculate_kl_divergence(dist_a, dist_a)             print(f\"  - KL(A || A): {kl_a_a:.4f} (should be 0)\")</p> <pre><code>        # Divergence of A from B (B is a good approximation of A)\n        kl_a_b = tools.calculate_kl_divergence(dist_a, dist_b)\n        print(f\"  - KL(A || B): {kl_a_b:.4f} (should be small)\")\n\n        # Divergence of A from C (C is a bad approximation of A)\n        kl_a_c = tools.calculate_kl_divergence(dist_a, dist_c)\n        print(f\"  - KL(A || C): {kl_a_c:.4f} (should be large)\")\n\n        # Note that KL divergence is asymmetric\n        kl_c_a = tools.calculate_kl_divergence(dist_c, dist_a)\n        print(f\"  - KL(C || A): {kl_c_a:.4f} (note: not equal to KL(A || C))\")\n</code></pre> Source code in <code>src\\ds_tool.py</code> <pre><code>@staticmethod\ndef calculate_kl_divergence(\n    p: Union[np.ndarray, List[float]],\n    q: Union[np.ndarray, List[float]],\n    base: Optional[float] = None,\n) -&gt; float:\n    \"\"\"\n    Calculates the Kullback-Leibler (KL) divergence between two distributions.\n\n    KL divergence D_KL(P || Q) measures how one probability distribution P\n    diverges from a second, expected probability distribution Q. It is\n    asymmetric.\n\n    Args:\n        p: A 1D array-like object for the \"true\" or reference distribution (P).\n        q: A 1D array-like object for the \"approximating\" distribution (Q).\n        base: The logarithmic base to use. Defaults to natural log (nats).\n\n    Returns:\n        The KL divergence as a float.\n\n    Raises:\n        ValueError: If input arrays have different lengths or contain negative values.\n\n    Usage:\n        print(\"\\nCalculating KL Divergence (D_KL(P || Q)):\")\n        # Divergence of a distribution from itself should be 0\n        kl_a_a = tools.calculate_kl_divergence(dist_a, dist_a)\n        print(f\"  - KL(A || A): {kl_a_a:.4f} (should be 0)\")\n\n        # Divergence of A from B (B is a good approximation of A)\n        kl_a_b = tools.calculate_kl_divergence(dist_a, dist_b)\n        print(f\"  - KL(A || B): {kl_a_b:.4f} (should be small)\")\n\n        # Divergence of A from C (C is a bad approximation of A)\n        kl_a_c = tools.calculate_kl_divergence(dist_a, dist_c)\n        print(f\"  - KL(A || C): {kl_a_c:.4f} (should be large)\")\n\n        # Note that KL divergence is asymmetric\n        kl_c_a = tools.calculate_kl_divergence(dist_c, dist_a)\n        print(f\"  - KL(C || A): {kl_c_a:.4f} (note: not equal to KL(A || C))\")\n    \"\"\"\n    p_arr = np.asarray(p, dtype=float)\n    q_arr = np.asarray(q, dtype=float)\n\n    if p_arr.shape != q_arr.shape:\n        raise ValueError(\"Input distributions P and Q must have the same shape.\")\n\n    if np.any(p_arr &lt; 0) or np.any(q_arr &lt; 0):\n        raise ValueError(\"Probabilities cannot be negative.\")\n\n    # Normalize both distributions\n    p_arr /= np.sum(p_arr)\n    q_arr /= np.sum(q_arr)\n\n    # Add a small epsilon to avoid division by zero or log(0)\n    # We only need to protect q from being zero where p is non-zero\n    p_arr += EPSILON\n    q_arr += EPSILON\n\n    if base is None:\n        return np.sum(p_arr * np.log(p_arr / q_arr))\n    else:\n        return np.sum(p_arr * (np.log(p_arr / q_arr) / np.log(base)))\n</code></pre>"},{"location":"api/ds_tool/#ds_tool.DSTools.category_stats","title":"<code>category_stats(df, col_name)</code>","text":"<p>Calculate and print categorical statistics for unique values analysis.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>Input DataFrame</p> required <code>col_name</code> <code>str</code> <p>Column name for statistics calculation</p> required Usage <p>tools = DSTools() tools.category_stats(df, 'category_column')</p> Source code in <code>src\\ds_tool.py</code> <pre><code>def category_stats(self, df: pd.DataFrame, col_name: str) -&gt; None:\n    \"\"\"\n    Calculate and print categorical statistics for unique values analysis.\n\n    Args:\n        df: Input DataFrame\n        col_name: Column name for statistics calculation\n\n    Usage:\n         tools = DSTools()\n         tools.category_stats(df, 'category_column')\n    \"\"\"\n    if col_name not in df.columns:\n        raise ValueError(f\"Column {col_name} not found in DataFrame\")\n\n    value_counts = df[col_name].value_counts()\n    percentage = df[col_name].value_counts(normalize=True) * 100\n\n    aggr_stats = pd.DataFrame(\n        {\n            \"uniq_names\": value_counts.index.tolist(),\n            \"amount_values\": value_counts.values.tolist(),\n            \"percentage\": percentage.values.tolist(),\n        }\n    )\n\n    aggr_stats.columns = pd.MultiIndex.from_product(\n        [[col_name], aggr_stats.columns]\n    )\n    print(aggr_stats)\n</code></pre>"},{"location":"api/ds_tool/#ds_tool.DSTools.chatterjee_correlation","title":"<code>chatterjee_correlation(x, y, standard_flag=True)</code>  <code>staticmethod</code>","text":"<p>Calculates Chatterjee's rank correlation coefficient (Xi).</p> <p>This coefficient is a non-parametric measure of dependence between two variables. It is asymmetric and ranges from 0 to 1, where a value close to 1 indicates that y is a function of x. It can capture non-linear relationships.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Union[ndarray, Series, List[float]]</code> <p>Array-like, the first variable (independent).</p> required <code>y</code> <code>Union[ndarray, Series, List[float]]</code> <p>Array-like, the second variable (dependent).</p> required <code>standard_flag</code> <code>bool</code> <p>bool flag which define type of calculation</p> <code>True</code> <p>Returns:</p> Type Description <code>float</code> <p>The Chatterjee's correlation coefficient, a float between 0 and 1.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the input arrays do not have the same length.</p> Usage <p>x = np.linspace(0, 10, 100) y_linear = 2 * x + 1 y_nonlinear = np.sin(x) tools = DSTools() print(f\"Linear correlation: {tools.chatterjee_correlation(x, y_linear):.4f}\") print(f\"Non-linear correlation: {tools.chatterjee_correlation(x, y_nonlinear):.4f}\")</p> Source code in <code>src\\ds_tool.py</code> <pre><code>@staticmethod\ndef chatterjee_correlation(\n    x: Union[np.ndarray, pd.Series, List[float]],\n    y: Union[np.ndarray, pd.Series, List[float]],\n    standard_flag: bool = True,\n) -&gt; float:\n    \"\"\"\n    Calculates Chatterjee's rank correlation coefficient (Xi).\n\n    This coefficient is a non-parametric measure of dependence between two\n    variables. It is asymmetric and ranges from 0 to 1, where a value\n    close to 1 indicates that y is a function of x. It can capture\n    non-linear relationships.\n\n    Args:\n        x: Array-like, the first variable (independent).\n        y: Array-like, the second variable (dependent).\n        standard_flag: bool flag which define type of calculation\n\n    Returns:\n        The Chatterjee's correlation coefficient, a float between 0 and 1.\n\n    Raises:\n        ValueError: If the input arrays do not have the same length.\n\n    Usage:\n         x = np.linspace(0, 10, 100)\n         y_linear = 2 * x + 1\n         y_nonlinear = np.sin(x)\n         tools = DSTools()\n         print(f\"Linear correlation: {tools.chatterjee_correlation(x, y_linear):.4f}\")\n         print(f\"Non-linear correlation: {tools.chatterjee_correlation(x, y_nonlinear):.4f}\")\n    \"\"\"\n    # 1. Convert inputs to NumPy arrays and validate\n    x_arr = np.asarray(x)\n    y_arr = np.asarray(y)\n\n    n = len(x_arr)\n    if n != len(y_arr):\n        raise ValueError(\"Input arrays x and y must have the same length.\")\n\n    if n &lt; 2:\n        return 0.0  # Correlation is undefined for less than 2 points\n\n    # 2. Get the ranks of y based on the sorted order of x\n    # argsort gives the indices that would sort x\n    x_order_indices = np.argsort(x_arr)\n\n    # Reorder y according to the sorted x\n    y_ordered_by_x = y_arr[x_order_indices]\n\n    # Calculate ranks of the reordered y. 'average' method handles ties.\n    # This replaces the dependency on pandas.Series.rank()\n    y_ranks = rankdata(y_ordered_by_x, method=\"average\")\n\n    # 3. Calculate the sum of absolute differences of consecutive ranks\n    # np.diff calculates the difference between adjacent elements\n    rank_diffs_sum = np.sum(np.abs(np.diff(y_ranks)))\n\n    # 4. Calculate Chatterjee's Xi coefficient\n    # The original formula is 1 - (3 * sum(|r_{i+1} - r_i|)) / (n^2 - 1)\n    # An equivalent and more stable formula is used below.\n    xi_orig = 1 - (n * rank_diffs_sum) / (\n        2 * np.sum(np.abs(y_ranks - np.mean(y_ranks)) ** 2)\n    )\n\n    xi = 1 - (3 * rank_diffs_sum) / (n**2 - 1) if standard_flag else xi_orig\n\n    return xi\n</code></pre>"},{"location":"api/ds_tool/#ds_tool.DSTools.check_NINF","title":"<code>check_NINF(data)</code>","text":"<p>Check DataFrame or array for NaN and infinite values.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>Union[DataFrame, ndarray]</code> <p>Input data to check</p> required Usage <p>tools = DSTools() tools.check_NINF(data)</p> Source code in <code>src\\ds_tool.py</code> <pre><code>def check_NINF(self, data: Union[pd.DataFrame, np.ndarray]) -&gt; None:\n    \"\"\"\n    Check DataFrame or array for NaN and infinite values.\n\n    Args:\n        data: Input data to check\n\n    Usage:\n         tools = DSTools()\n         tools.check_NINF(data)\n    \"\"\"\n    if isinstance(data, pd.DataFrame):\n        has_nan = data.isnull().any().any()\n        has_inf = np.isinf(data.select_dtypes(include=[np.number])).any().any()\n    else:\n        has_nan = np.isnan(data).any()\n        has_inf = np.isinf(data).any()\n\n    if not has_nan and not has_inf:\n        print(\"Dataset has no NaN or infinite values\")\n    elif has_nan and not has_inf:\n        print(\"Dataset has NaN values but no infinite values\")\n    elif not has_nan and has_inf:\n        print(\"Dataset has infinite values but no NaN values\")\n    else:\n        print(\"Dataset has both NaN and infinite values\")\n</code></pre>"},{"location":"api/ds_tool/#ds_tool.DSTools.compute_metrics","title":"<code>compute_metrics(y_true, y_predict, y_predict_proba, config=None)</code>","text":"<p>Calculate main pre-selected classification metrics.</p> <p>Parameters:</p> Name Type Description Default <code>y_true</code> <code>ndarray</code> <p>True labels</p> required <code>y_predict</code> <code>ndarray</code> <p>Predicted labels</p> required <code>y_predict_proba</code> <code>ndarray</code> <p>Predicted probabilities</p> required <code>config</code> <code>Optional[MetricsConfig]</code> <p>Configuration for metrics computation</p> <code>None</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>DataFrame with calculated metrics</p> Usage <p>from ds_tool import DSTools, MetricsConfig tools = DSTools() metrics = tools.compute_metrics(y_test, y_pred, y_pred_proba)</p> Source code in <code>src\\ds_tool.py</code> <pre><code>def compute_metrics(\n    self,\n    y_true: np.ndarray,\n    y_predict: np.ndarray,\n    y_predict_proba: np.ndarray,\n    config: Optional[MetricsConfig] = None,\n) -&gt; pd.DataFrame:\n    \"\"\"\n    Calculate main pre-selected classification metrics.\n\n    Args:\n        y_true: True labels\n        y_predict: Predicted labels\n        y_predict_proba: Predicted probabilities\n        config: Configuration for metrics computation\n\n    Returns:\n        DataFrame with calculated metrics\n\n    Usage:\n        from ds_tool import DSTools, MetricsConfig\n        tools = DSTools()\n        metrics = tools.compute_metrics(y_test, y_pred, y_pred_proba)\n    \"\"\"\n    if config is None:\n        config = MetricsConfig()\n\n    metrics_dict = {}\n\n    # Average Precision Score\n    aps = average_precision_score(y_true, y_predict_proba) * 100\n    metrics_dict[\"Average_precision, %\"] = round(aps, 2)\n\n    if config.print_values:\n        print(f\"Average_precision = {aps:.3f} %\")\n\n    # Balanced Accuracy Score\n    bas = balanced_accuracy_score(y_true, y_predict) * 100\n    metrics_dict[\"Balanced_accuracy, %\"] = round(bas, 2)\n\n    if config.print_values:\n        print(f\"Balanced_accuracy = {bas:.3f} %\")\n\n    # Likelihood Ratios\n    clr = class_likelihood_ratios(y_true, y_predict)\n    metrics_dict[\"Likelihood_ratios+\"] = clr[0]\n    metrics_dict[\"Likelihood_ratios-\"] = clr[1]\n\n    if config.print_values:\n        print(\n            f\"Likelihood_ratios+ = {clr[0]:.3f}\\nLikelihood_ratios- = {clr[1]:.3f}\"\n        )\n\n    # Cohen's Kappa Score\n    cks = cohen_kappa_score(y_true, y_predict) * 100\n    metrics_dict[\"Kappa_score, %\"] = round(cks, 2)\n\n    if config.print_values:\n        print(f\"Kappa_score = {cks:.3f} %\")\n\n    # Hamming Loss\n    hl = hamming_loss(y_true, y_predict) * 100\n    metrics_dict[\"Incor_pred_labels (hamming_loss), %\"] = round(hl, 2)\n\n    if config.print_values:\n        print(f\"Incor_pred_labels (hamming_loss) = {hl:.3f} %\")\n\n    # Jaccard Score\n    hs = jaccard_score(y_true, y_predict) * 100\n    metrics_dict[\"Jaccard_similarity, %\"] = round(hs, 2)\n\n    if config.print_values:\n        print(f\"Jaccard_similarity = {hs:.3f} %\")\n\n    # Log Loss\n    ls = log_loss(y_true, y_predict_proba)\n    metrics_dict[\"Cross_entropy_loss\"] = ls\n\n    if config.print_values:\n        print(f\"Cross_entropy_loss = {ls:.3f}\")\n\n    # Correlation Coefficient\n    cc = np.corrcoef(y_true, y_predict)[0][1] * 100\n    metrics_dict[\"Coef_correlation, %\"] = round(cc, 2)\n\n    if config.print_values:\n        print(f\"Coef_correlation = {cc:.3f} %\")\n\n    # Error visualization\n    if config.error_vis:\n        fpr, fnr, thresholds = det_curve(y_true, y_predict_proba)\n        plt.plot(thresholds, fpr, label=\"False Positive Rate (FPR)\")\n        plt.plot(thresholds, fnr, label=\"False Negative Rate (FNR)\")\n        plt.title(\"Error Rates vs Threshold Levels\")\n        plt.xlabel(\"Threshold Level\")\n        plt.ylabel(\"Error Rate\")\n        plt.legend()\n        plt.grid(True)\n        plt.show()\n\n    return pd.DataFrame([metrics_dict])\n</code></pre>"},{"location":"api/ds_tool/#ds_tool.DSTools.corr_matrix","title":"<code>corr_matrix(df, config=None)</code>","text":"<p>Calculate and visualize correlation matrix.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>Input DataFrame with numerical columns</p> required <code>config</code> <code>Optional[CorrelationConfig]</code> <p>Configuration for correlation matrix visualization</p> <code>None</code> Usage <p>from ds_tool import DSTools, CorrelationConfig tools = DSTools() tools.corr_matrix(df, CorrelationConfig(font_size=12))</p> Source code in <code>src\\ds_tool.py</code> <pre><code>def corr_matrix(\n    self, df: pd.DataFrame, config: Optional[CorrelationConfig] = None\n) -&gt; None:\n    \"\"\"\n    Calculate and visualize correlation matrix.\n\n    Args:\n        df: Input DataFrame with numerical columns\n        config: Configuration for correlation matrix visualization\n\n    Usage:\n         from ds_tool import DSTools, CorrelationConfig\n         tools = DSTools()\n         tools.corr_matrix(df, CorrelationConfig(font_size=12))\n    \"\"\"\n    if config is None:\n        config = CorrelationConfig()\n\n    # Calculate correlation matrix\n    corr = df.corr(method=config.build_method)\n    mask = np.triu(np.ones_like(corr, dtype=bool))\n\n    # Determine figure size based on number of columns\n    n_cols = len(df.columns)\n\n    if n_cols &lt; 5:\n        fig_size = (8, 8)\n    elif n_cols &lt; 9:\n        fig_size = (10, 10)\n    elif n_cols &lt; 15:\n        fig_size = (22, 22)\n    else:\n        fig_size = config.image_size\n\n    fig, ax = plt.subplots(figsize=fig_size)\n\n    # Create heatmap\n    ax = sns.heatmap(\n        corr,\n        annot=True,\n        annot_kws={\"size\": config.font_size},\n        fmt=\".3f\",\n        center=0,\n        linewidths=1.0,\n        linecolor=\"black\",\n        square=True,\n        cmap=sns.diverging_palette(20, 220, n=100),\n        mask=mask,\n    )\n\n    # Customize x-axis\n    ax.tick_params(\n        axis=\"x\",\n        which=\"major\",\n        direction=\"inout\",\n        length=20,\n        width=4,\n        color=\"m\",\n        pad=10,\n        labelsize=16,\n        labelcolor=\"b\",\n        bottom=True,\n        top=True,\n        labelbottom=True,\n        labeltop=True,\n        labelrotation=85,\n    )\n\n    # Customize y-axis\n    ax.tick_params(\n        axis=\"y\",\n        which=\"major\",\n        direction=\"inout\",\n        length=20,\n        width=4,\n        color=\"m\",\n        pad=10,\n        labelsize=16,\n        labelcolor=\"r\",\n        left=True,\n        right=False,\n        labelleft=True,\n        labelright=False,\n        labelrotation=0,\n    )\n\n    ax.set_yticklabels(\n        ax.get_yticklabels(), rotation=0, fontsize=16, verticalalignment=\"center\"\n    )\n\n    plt.title(\n        f\"Correlation ({config.build_method}) matrix for selected features\",\n        fontsize=20,\n    )\n    plt.tight_layout()\n    plt.show()\n</code></pre>"},{"location":"api/ds_tool/#ds_tool.DSTools.describe_categorical","title":"<code>describe_categorical(df)</code>","text":"<p>Detailed description of categorical columns.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>Input DataFrame</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>DataFrame with categorical statistics</p> Usage <p>tools = DSTools() cat_stats = tools.describe_categorical(df)</p> Source code in <code>src\\ds_tool.py</code> <pre><code>def describe_categorical(self, df: pd.DataFrame) -&gt; pd.DataFrame:\n    \"\"\"\n    Detailed description of categorical columns.\n\n    Args:\n        df: Input DataFrame\n\n    Returns:\n        DataFrame with categorical statistics\n\n    Usage:\n         tools = DSTools()\n         cat_stats = tools.describe_categorical(df)\n    \"\"\"\n    # 1. Select columns with types 'object', 'category', 'string'\n    categorical_cols = df.select_dtypes(\n        include=[\"object\", \"category\", \"string\"]\n    ).columns.tolist()\n\n    # 2. Find columns that ONLY consist of NaN (they can be of numeric type)\n    all_nan_cols = df.columns[df.isnull().all()].tolist()\n\n    # 3. Combine both lists and remove duplicates\n    cols_to_process = sorted(list(set(categorical_cols + all_nan_cols)))\n\n    if not cols_to_process:\n        return pd.DataFrame()\n\n    # 4. Get basic descriptive statistics\n    description = df[cols_to_process].describe(include=\"all\").T\n\n    # 5. Calculate the percentage of missing data\n    missing_percent = (df[cols_to_process].isnull().sum() / len(df) * 100).round(1)\n\n    # 6. Assemble the final DataFrame\n    result_df = description\n    result_df[\"missing (%)\"] = missing_percent\n\n    # 7. Order and clear the columns\n    # Remove 'count', as it duplicates the information about missing data\n    if \"count\" in result_df.columns:\n        result_df = result_df.drop(columns=\"count\")\n\n    final_cols_order = [\"missing (%)\", \"unique\", \"top\", \"freq\"]\n\n    # Leave only those columns from our ideal list that actually exist\n    existing_cols = [col for col in final_cols_order if col in result_df.columns]\n\n    return result_df[existing_cols]\n</code></pre>"},{"location":"api/ds_tool/#ds_tool.DSTools.describe_numeric","title":"<code>describe_numeric(df)</code>","text":"<p>Detailed description of numerical columns.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>Input DataFrame</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>DataFrame with numerical statistics</p> Usage <p>tools = DSTools() num_stats = tools.describe_numeric(df)</p> Source code in <code>src\\ds_tool.py</code> <pre><code>def describe_numeric(self, df: pd.DataFrame) -&gt; pd.DataFrame:\n    \"\"\"\n    Detailed description of numerical columns.\n\n    Args:\n        df: Input DataFrame\n\n    Returns:\n        DataFrame with numerical statistics\n\n    Usage:\n         tools = DSTools()\n         num_stats = tools.describe_numeric(df)\n    \"\"\"\n    numeric_cols = df.select_dtypes(include=[np.number]).columns\n    if len(numeric_cols) == 0:\n        return pd.DataFrame()\n\n    numeric_df = df[numeric_cols]\n    description = numeric_df.describe()\n\n    result_data = {\n        \"sum\": numeric_df.sum(),\n        \"missing (%)\": np.round(numeric_df.isnull().sum() / len(df) * 100, 1),\n        \"median\": numeric_df.median(),\n        \"skew\": numeric_df.skew(),\n        \"kurtosis\": numeric_df.kurtosis(),\n    }\n\n    # Add description statistics\n    for stat in description.index:\n        if stat != \"count\":\n            result_data[stat] = description.loc[stat]\n\n    return pd.DataFrame(result_data, index=numeric_cols)\n</code></pre>"},{"location":"api/ds_tool/#ds_tool.DSTools.df_stats","title":"<code>df_stats(df, return_format='dict', detailed=True)</code>","text":"<p>Provide quick overview of DataFrame structure.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>Input DataFrame</p> required <code>return_format</code> <code>str</code> <p>Format of return ('dict' or 'dataframe')</p> <code>'dict'</code> <code>detailed</code> <code>bool</code> <p>Include additional statistics</p> <code>True</code> <p>Returns:</p> Type Description <code>Union[dict, DataFrame]</code> <p>dict or DataFrame with statistics</p> Source code in <code>src\\ds_tool.py</code> <pre><code>def df_stats(\n    self, df: pd.DataFrame, return_format: str = \"dict\", detailed: bool = True\n) -&gt; Union[dict, pd.DataFrame]:\n    \"\"\"\n    Provide quick overview of DataFrame structure.\n\n    Args:\n        df: Input DataFrame\n        return_format: Format of return ('dict' or 'dataframe')\n        detailed: Include additional statistics\n\n    Returns:\n        dict or DataFrame with statistics\n    \"\"\"\n    stats = {\n        \"columns\": df.shape[1],\n        \"rows\": df.shape[0],\n        \"missing_percent\": np.round(df.isnull().sum().sum() / df.size * 100, 1),\n        \"memory_mb\": np.round(df.memory_usage(deep=True).sum() / 10**6, 1),\n    }\n\n    if detailed:\n        stats.update(\n            {\n                \"numeric_columns\": df.select_dtypes(include=[np.number]).shape[1],\n                \"categorical_columns\": df.select_dtypes(\n                    include=[\"object\", \"category\"]\n                ).shape[1],\n                \"datetime_columns\": df.select_dtypes(include=[\"datetime\"]).shape[1],\n                \"duplicated_rows\": df.duplicated().sum(),\n                \"total_missing_values\": df.isnull().sum().sum(),\n            }\n        )\n\n    if return_format.lower() == \"dataframe\":\n        return pd.DataFrame(list(stats.items()), columns=[\"metric\", \"value\"])\n    else:\n        return stats\n</code></pre>"},{"location":"api/ds_tool/#ds_tool.DSTools.evaluate_classification","title":"<code>evaluate_classification(true_labels, pred_probs, threshold=0.5, figsize=(16, 7))</code>  <code>staticmethod</code>","text":"<p>Calculates, prints, and visualizes metrics for a binary classification model.</p> <p>This \"all-in-one\" method provides a complete performance summary, including key scalar metrics, a classification report, a confusion matrix, and plots for ROC and Precision-Recall curves.</p> <p>Parameters:</p> Name Type Description Default <code>true_labels</code> <code>ndarray</code> <p>Array of true binary labels (0 or 1).</p> required <code>pred_probs</code> <code>ndarray</code> <p>Array of predicted probabilities for the positive class.</p> required <code>threshold</code> <code>float</code> <p>The cutoff to convert probabilities into binary predictions.</p> <code>0.5</code> <code>figsize</code> <code>Tuple[int, int]</code> <p>The size of the figure for the plots.</p> <code>(16, 7)</code> <p>Returns:</p> Type Description <code>Dict[str, Any]</code> <p>A dictionary containing the calculated metrics for programmatic use.</p> Source code in <code>src\\ds_tool.py</code> <pre><code>@staticmethod\ndef evaluate_classification(\n    true_labels: np.ndarray,\n    pred_probs: np.ndarray,\n    threshold: float = 0.5,\n    figsize: Tuple[int, int] = (16, 7),\n) -&gt; Dict[str, Any]:\n    \"\"\"\n    Calculates, prints, and visualizes metrics for a binary classification model.\n\n    This \"all-in-one\" method provides a complete performance summary, including\n    key scalar metrics, a classification report, a confusion matrix, and\n    plots for ROC and Precision-Recall curves.\n\n    Args:\n        true_labels: Array of true binary labels (0 or 1).\n        pred_probs: Array of predicted probabilities for the positive class.\n        threshold: The cutoff to convert probabilities into binary predictions.\n        figsize: The size of the figure for the plots.\n\n    Returns:\n        A dictionary containing the calculated metrics for programmatic use.\n    \"\"\"\n    # --- 1. Input Validation ---\n    if not isinstance(true_labels, np.ndarray) or not isinstance(\n        pred_probs, np.ndarray\n    ):\n        raise TypeError(\"Inputs true_labels and pred_probs must be NumPy arrays.\")\n\n    if true_labels.shape != pred_probs.shape:\n        raise ValueError(\"Shape of true_labels and pred_probs must match.\")\n\n    # --- 2. Threshold-dependent Metrics ---\n    pred_labels = (pred_probs &gt;= threshold).astype(int)\n\n    accuracy = accuracy_score(true_labels, pred_labels)\n    report_dict = classification_report(\n        true_labels, pred_labels, output_dict=True, zero_division=0\n    )\n    conf_matrix = confusion_matrix(true_labels, pred_labels)\n\n    # --- 3. Threshold-independent Metrics ---\n    fpr, tpr, _ = roc_curve(true_labels, pred_probs)\n    ks = max(tpr - fpr)\n    roc_auc = auc(fpr, tpr)\n    avg_precision = average_precision_score(true_labels, pred_probs)\n    precision, recall, _ = precision_recall_curve(true_labels, pred_probs)\n\n    # --- 4. Console Output ---\n    print(\"*\" * 60)\n    print(\n        f\"{'CLASSIFICATION METRICS SUMMARY (Threshold = ' + str(threshold) + ')':^60}\"\n    )\n    print(\"*\" * 60)\n    print(f\"  - Accuracy          : {accuracy:.4f}\")\n    print(f\"  - ROC AUC           : {roc_auc:.4f}\")\n    print(f\"  - Average Precision : {avg_precision:.4f}\")\n    print(f\"  - Kolmogorov-Smirnov : {ks:.4f}\")\n    print(\"-\" * 60)\n\n    print(f\"\\n{'Classification Report':^60}\\n\")\n    report_df = pd.DataFrame(report_dict).transpose()\n    print(report_df.round(4))\n\n    print(f\"\\n{'Confusion Matrix':^60}\\n\")\n    print(\n        pd.DataFrame(\n            conf_matrix,\n            index=[\"Actual 0\", \"Actual 1\"],\n            columns=[\"Predicted 0\", \"Predicted 1\"],\n        )\n    )\n    print(\"*\" * 60)\n\n    # --- 5. Visualization ---\n    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=figsize)\n    plt.style.use(\"seaborn-v0_8-whitegrid\")\n\n    # Precision-Recall Curve\n    ax1.step(recall, precision, color=\"b\", alpha=0.8, where=\"post\")\n    ax1.fill_between(recall, precision, step=\"post\", alpha=0.2, color=\"b\")\n    ax1.set_xlabel(\"Recall\", fontsize=14)\n    ax1.set_ylabel(\"Precision\", fontsize=14)\n    ax1.set_ylim([0.0, 1.05])\n    ax1.set_xlim([0.0, 1.0])\n    ax1.set_title(f\"Precision-Recall Curve\\nAP = {avg_precision:.2f}\", fontsize=16)\n\n    # ROC Curve\n    ax2.plot(\n        fpr,\n        tpr,\n        color=\"darkorange\",\n        lw=2,\n        label=f\"ROC curve (AUC = {roc_auc:.2f}, KS = {ks:.2f})\",\n    )\n    ax2.plot([0, 1], [0, 1], color=\"navy\", lw=2, linestyle=\"--\")\n    ax2.fill_between(\n        fpr,\n        tpr,\n        fpr,\n        where=(tpr &gt;= fpr),\n        alpha=0.3,\n        color=\"green\",\n        interpolate=True,\n        label=\"Above random\",\n    )\n    ax2.set_xlim([0.0, 1.0])\n    ax2.set_ylim([0.0, 1.05])\n    ax2.set_xlabel(\"False Positive Rate\", fontsize=14)\n    ax2.set_ylabel(\"True Positive Rate\", fontsize=14)\n    ax2.set_title(\"Receiver Operating Characteristic (ROC)\", fontsize=16)\n    ax2.legend(loc=\"lower right\", fontsize=12)\n\n    plt.tight_layout()\n    plt.show()\n\n    # --- 6. Return Metrics Dictionary ---\n    return {\n        \"threshold\": threshold,\n        \"accuracy\": accuracy,\n        \"roc_auc\": roc_auc,\n        \"average_precision\": avg_precision,\n        \"Kolmogorov-Smirnov\": ks,\n        \"classification_report\": report_dict,\n        \"confusion_matrix\": conf_matrix,\n    }\n</code></pre>"},{"location":"api/ds_tool/#ds_tool.DSTools.function_list","title":"<code>function_list()</code>","text":"<p>Parses the list of available tools (the 'Agenda') from the class docstring as a formatted table (Pandas DataFrame).</p> <p>pd.DataFrame: A DataFrame with 'Function Name' and 'Description'               columns. Returns an empty DataFrame if the 'Agenda'               section is not found.</p> Usage <p>pd.set_option('display.max_colwidth', 200) tools = DSTools() tools.function_list()</p> Source code in <code>src\\ds_tool.py</code> <pre><code>def function_list(self) -&gt; pd.DataFrame:\n    \"\"\"\n    Parses the list of available tools (the 'Agenda') from the class\n    docstring as a formatted table (Pandas DataFrame).\n\n    Returns:\n    pd.DataFrame: A DataFrame with 'Function Name' and 'Description'\n                  columns. Returns an empty DataFrame if the 'Agenda'\n                  section is not found.\n\n    Usage:\n        pd.set_option('display.max_colwidth', 200)\n        tools = DSTools()\n        tools.function_list()\n    \"\"\"\n    # 1. Get the main docstring of the class\n    doc = self.__class__.__doc__\n\n    if not doc:\n        print(\"Warning: No documentation found for this class.\")\n        return pd.DataFrame(columns=[\"Function Name\", \"Description\"])\n\n    # 2. Find the 'Agenda' section\n    match = re.search(r\"Agenda:\\s*---+\\s*(.*)\", doc, re.S)\n\n    if not match:\n        print(\"Warning: No 'Agenda' section found in the class documentation.\")\n        return pd.DataFrame(columns=[\"Function Name\", \"Description\"])\n\n    # 3. Parse the content with the robust regex method\n    agenda_content = match.group(1).strip()\n    lines = agenda_content.split(\"\\n\")\n\n    tools_data = []\n    current_entry = None\n    entry_pattern = re.compile(r\"^\\s*([a-zA-Z0-9_]+):\\s*(.*)\")\n\n    for line in lines:\n        m = entry_pattern.match(line)\n        if m:\n            if current_entry:\n                current_entry[\"Description\"] = \" \".join(\n                    current_entry[\"Description\"]\n                ).strip()\n                tools_data.append(current_entry)\n\n            func_name = m.group(1)\n            desc_part = m.group(2).strip()\n            current_entry = {\n                \"Function Name\": func_name,\n                \"Description\": [desc_part] if desc_part else [],\n            }\n        elif current_entry and line.strip():\n            current_entry[\"Description\"].append(line.strip())\n\n    if current_entry:\n        current_entry[\"Description\"] = \" \".join(\n            current_entry[\"Description\"]\n        ).strip()\n        tools_data.append(current_entry)\n\n    # 4. Create and return the DataFrame\n    if not tools_data:\n        print(\"Warning: No tools found in the Agenda.\")\n        return pd.DataFrame(columns=[\"Function Name\", \"Description\"])\n    out_df = pd.DataFrame(tools_data).iloc[1:]\n\n    with pd.option_context(\n        \"display.max_colwidth\",\n        200,  # \u041c\u0430\u043a\u0441. \u0448\u0438\u0440\u0438\u043d\u0430 \u043a\u043e\u043b\u043e\u043d\u043a\u0438 (\u0432 \u0441\u0438\u043c\u0432\u043e\u043b\u0430\u0445)\n        \"display.width\",\n        350,  # \u041e\u0431\u0449\u0430\u044f \u0448\u0438\u0440\u0438\u043d\u0430 \u0432\u044b\u0432\u043e\u0434\u0430\n        \"display.colheader_justify\",\n        \"center\",  # \u0412\u044b\u0440\u0430\u0432\u043d\u0438\u0432\u0430\u043d\u0438\u0435 \u0437\u0430\u0433\u043e\u043b\u043e\u0432\u043a\u043e\u0432 \u043f\u043e \u043b\u0435\u0432\u043e\u043c\u0443 \u043a\u0440\u0430\u044e\n    ):\n        return out_df\n</code></pre>"},{"location":"api/ds_tool/#ds_tool.DSTools.generate_alphanum_codes","title":"<code>generate_alphanum_codes(n, length=8)</code>  <code>staticmethod</code>","text":"<pre><code>    Generates an array of random alphanumeric codes.\n\n    This method is optimized for performance by using NumPy vectorized operations.\n\n    Args:\n        n: The number of codes to generate.\n        length: The length of each code.\n\n    Returns:\n        A NumPy array of strings, where each string is a random code.\n\n    Usage:\n        tools = DSTools()\n        codes = tools.generate_alphanum_codes(5, length=10)\n        print(f\"Generated codes:\n</code></pre> <p>{codes}\")</p> Source code in <code>src\\ds_tool.py</code> <pre><code>@staticmethod\ndef generate_alphanum_codes(n: int, length: int = 8) -&gt; np.ndarray:\n    \"\"\"\n    Generates an array of random alphanumeric codes.\n\n    This method is optimized for performance by using NumPy vectorized operations.\n\n    Args:\n        n: The number of codes to generate.\n        length: The length of each code.\n\n    Returns:\n        A NumPy array of strings, where each string is a random code.\n\n    Usage:\n        tools = DSTools()\n        codes = tools.generate_alphanum_codes(5, length=10)\n        print(f\"Generated codes:\\n{codes}\")\n    \"\"\"\n    if n &lt; 0 or length &lt; 0:\n        raise ValueError(\"Number of codes (n) and length must be non-negative.\")\n\n    if length == 0:\n        return np.full(n, \"\", dtype=str)\n\n    # A clean, non-repeating alphabet\n    alphabet = \"0123456789ABCDEFGHIJKLMNOPQRSTUVWXYZ\"\n    alphabet_len = len(alphabet)\n\n    # Generate all random indices at once\n    random_indices = np.random.randint(0, alphabet_len, size=(n, length))\n\n    # Use NumPy's advanced indexing to get characters\n    # .view('S1') treats each character as a 1-byte string\n    # .reshape converts back to the desired shape\n    codes_as_chars = np.array(list(alphabet), dtype=\"S1\")[random_indices]\n\n    # .view('S{length}') joins the characters in each row into a single string\n    # This is a highly optimized, low-level NumPy operation\n    codes_as_bytes = codes_as_chars.view(f\"S{length}\")\n\n    # Decode from bytes to a standard UTF-8 string array\n    return np.char.decode(codes_as_bytes.flatten(), \"utf-8\")\n</code></pre>"},{"location":"api/ds_tool/#ds_tool.DSTools.generate_distribution","title":"<code>generate_distribution(config)</code>","text":"<p>Generates a distribution matching the provided statistical metrics.</p> <p>This function creates a distribution by generating a base dataset with a shape defined by kurtosis, adds outliers, and then iteratively scales and shifts the data to match the target mean and standard deviation within a specified accuracy threshold.</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <code>DistributionConfig</code> <p>A Pydantic model instance containing all configuration parameters.</p> required <p>Returns:</p> Type Description <code>ndarray</code> <p>A NumPy array of numerical values with the specified properties.</p> Usage <p>tools = DSTools() config = DistributionConfig(</p> <pre><code>...     mean=100, median=95, std=15, min_val=50, max_val=200,\n...     skewness=0.5, kurtosis=3.5, n=1000\n... )\n data = tools.generate_distribution(config)\n print(f'Generated Mean: {np.mean(data):.2f}, Std: {np.std(data):.2f}')\n</code></pre> Source code in <code>src\\ds_tool.py</code> <pre><code>def generate_distribution(self, config: DistributionConfig) -&gt; np.ndarray:\n    \"\"\"\n    Generates a distribution matching the provided statistical metrics.\n\n    This function creates a distribution by generating a base dataset with a\n    shape defined by kurtosis, adds outliers, and then iteratively scales\n    and shifts the data to match the target mean and standard deviation\n    within a specified accuracy threshold.\n\n    Args:\n        config: A Pydantic model instance containing all configuration parameters.\n\n    Returns:\n        A NumPy array of numerical values with the specified properties.\n\n    Usage:\n         tools = DSTools()\n         config = DistributionConfig(\n        ...     mean=100, median=95, std=15, min_val=50, max_val=200,\n        ...     skewness=0.5, kurtosis=3.5, n=1000\n        ... )\n         data = tools.generate_distribution(config)\n         print(f'Generated Mean: {np.mean(data):.2f}, Std: {np.std(data):.2f}')\n    \"\"\"\n    if not self.validate_moments(config.std, config.skewness, config.kurtosis):\n        raise ValueError(\"Invalid statistical moments\")\n    if config.min_val &gt;= config.max_val:\n        raise ValueError(\"max_val must be greater than min_val\")\n\n    num_outliers = int(config.n * config.outlier_ratio)\n    num_base = config.n - num_outliers\n\n    # --- 1. Generate Base Distribution ---\n    # Generate a base distribution with a shape influenced by kurtosis.\n    # Student's t-distribution is used for heavy tails (kurtosis &gt; 3).\n    if config.kurtosis &gt; 3.5:\n        # Lower degrees of freedom lead to heavier tails\n        df = max(1, int(10 / (config.kurtosis - 2.5)))\n        base_data = stats.t.rvs(df=df, size=num_base)\n    else:\n        base_data = np.random.standard_normal(size=num_base)\n\n    # --- 2. Add Outliers ---\n    # Generate outliers to further influence the tails.\n    if num_outliers &gt; 0:\n        # Outliers are generated with a larger variance to be distinct.\n        outlier_scale = config.std * (1 + config.kurtosis / 3)\n        outliers = np.random.normal(loc=0, scale=outlier_scale, size=num_outliers)\n        data = np.concatenate([base_data, outliers])\n    else:\n        data = base_data\n\n    np.random.shuffle(data)\n\n    # --- 3. Iterative Scaling and Shifting ---\n    # Iteratively adjust the data to match the target mean and std.\n    # This is more stable than trying to adjust all moments at once.\n    max_iterations = 50\n\n    for _ in range(max_iterations):\n        current_mean = np.mean(data)\n        current_std = np.std(data, ddof=1)\n\n        # Check for convergence\n        mean_ok = abs(current_mean - config.mean) &lt; (\n            abs(config.mean) * config.accuracy_threshold\n        )\n        std_ok = abs(current_std - config.std) &lt; (\n            config.std * config.accuracy_threshold\n        )\n\n        if mean_ok and std_ok:\n            break\n\n        # Rescale and shift the data\n        if current_std &gt; EPSILON:\n            data = config.mean + (data - current_mean) * (config.std / current_std)\n        else:\n            # Handle case where all values are the same\n            data = np.full_like(data, config.mean)\n\n    # --- 4. Final Adjustments ---\n    # Clip data to ensure it's within the min/max bounds\n    data = np.clip(data, config.min_val, config.max_val)\n\n    # Ensure min and max values are present in the final distribution\n    # This can slightly alter the final moments but guarantees the range.\n    if data.min() &gt; config.min_val:\n        data[np.argmin(data)] = config.min_val\n    if data.max() &lt; config.max_val:\n        data[np.argmax(data)] = config.max_val\n\n    return data\n</code></pre>"},{"location":"api/ds_tool/#ds_tool.DSTools.generate_distribution_from_metrics","title":"<code>generate_distribution_from_metrics(n, metrics, int_flag=True, output_as='numpy', max_iterations=100)</code>","text":"<p>Generates a synthetic distribution matching given statistical metrics.</p> <p>This function uses an iterative approach to create a distribution that approximates the properties specified in the DistributionConfig.</p> <p>Parameters:</p> Name Type Description Default <code>n</code> <code>int</code> <p>Number of values to generate.</p> required <code>metrics</code> <code>Union[DistributionConfig, Dict[str, float]]</code> <p>A Pydantic <code>DistributionConfig</code> instance OR a dictionary      with the target statistical properties.</p> required <code>int_flag</code> <code>bool</code> <p>If True, returns integer values; otherwise, floats.</p> <code>True</code> <code>output_as</code> <code>Literal['numpy', 'pandas', 'polars']</code> <p>The desired output format ('numpy', 'pandas', or 'polars').</p> <code>'numpy'</code> <code>max_iterations</code> <code>int</code> <p>int value is a number of iterations for tries.</p> <code>100</code> <p>Returns:</p> Type Description <code>Union[ndarray, Series, Series]</code> <p>An array or Series of generated values.</p> Usage <p>tools = DSTools() try:     metrics_dict = DistributionConfig(         mean=1042,         median=330,         std=1500,         min_val=1,         max_val=120000,         skewness=13.2,         kurtosis=245,         n=10000,         accuracy_threshold=0.05,         outlier_ratio=0.05     )</p> <pre><code># 2. Generate the data using the metrics object\ngenerated_data = tools.generate_distribution_from_metrics(\n    n=1000,\n    metrics=metrics_dict,\n    int_flag=True,\n    output_as='numpy'\n)\n\n# 3. Analyze the result\nprint(\"--- Target vs. Actual Statistics ---\")\nprint(f\"Target Mean: {metrics.mean}, Actual Mean: {np.mean(generated_data):.2f}\")\nprint(f\"Target Median: {metrics.median}, Actual Median: {np.median(generated_data):.2f}\")\nprint(f\"Target Std: {metrics.std}, Actual Std: {np.std(generated_data):.2f}\")\nprint(f\"Target Skew: {metrics.skewness}, Actual Skew: {stats.skew(generated_data):.2f}\")\nprint(f\"Target Kurtosis: {metrics.kurtosis}, Actual Kurtosis: {stats.kurtosis(generated_data, fisher=False):.2f}\")\nprint(f\"Target Min: {metrics.min_val}, Actual Min: {np.min(generated_data):.2f}\")\nprint(f\"Target Max: {metrics.max_val}, Actual Max: {np.max(generated_data):.2f}\")\n</code></pre> <p>except ValueError as e:     print(f\"Error during configuration or generation: {e}\")</p> Source code in <code>src\\ds_tool.py</code> <pre><code>def generate_distribution_from_metrics(\n    self,\n    n: int,\n    metrics: Union[DistributionConfig, Dict[str, float]],\n    int_flag: bool = True,\n    output_as: Literal[\"numpy\", \"pandas\", \"polars\"] = \"numpy\",\n    max_iterations: int = 100,\n) -&gt; Union[np.ndarray, pd.Series, pl.Series]:\n    \"\"\"\n    Generates a synthetic distribution matching given statistical metrics.\n\n    This function uses an iterative approach to create a distribution that\n    approximates the properties specified in the DistributionConfig.\n\n    Args:\n        n: Number of values to generate.\n        metrics: A Pydantic `DistributionConfig` instance OR a dictionary\n                 with the target statistical properties.\n        int_flag: If True, returns integer values; otherwise, floats.\n        output_as: The desired output format ('numpy', 'pandas', or 'polars').\n        max_iterations: int value is a number of iterations for tries.\n\n    Returns:\n        An array or Series of generated values.\n\n    Usage:\n        tools = DSTools()\n        try:\n            metrics_dict = DistributionConfig(\n                mean=1042,\n                median=330,\n                std=1500,\n                min_val=1,\n                max_val=120000,\n                skewness=13.2,\n                kurtosis=245,\n                n=10000,\n                accuracy_threshold=0.05,\n                outlier_ratio=0.05\n            )\n\n            # 2. Generate the data using the metrics object\n            generated_data = tools.generate_distribution_from_metrics(\n                n=1000,\n                metrics=metrics_dict,\n                int_flag=True,\n                output_as='numpy'\n            )\n\n            # 3. Analyze the result\n            print(\"--- Target vs. Actual Statistics ---\")\n            print(f\"Target Mean: {metrics.mean}, Actual Mean: {np.mean(generated_data):.2f}\")\n            print(f\"Target Median: {metrics.median}, Actual Median: {np.median(generated_data):.2f}\")\n            print(f\"Target Std: {metrics.std}, Actual Std: {np.std(generated_data):.2f}\")\n            print(f\"Target Skew: {metrics.skewness}, Actual Skew: {stats.skew(generated_data):.2f}\")\n            print(f\"Target Kurtosis: {metrics.kurtosis}, Actual Kurtosis: {stats.kurtosis(generated_data, fisher=False):.2f}\")\n            print(f\"Target Min: {metrics.min_val}, Actual Min: {np.min(generated_data):.2f}\")\n            print(f\"Target Max: {metrics.max_val}, Actual Max: {np.max(generated_data):.2f}\")\n\n        except ValueError as e:\n            print(f\"Error during configuration or generation: {e}\")\n    \"\"\"\n    if isinstance(metrics, dict):\n        try:\n            config = DistributionConfig(**metrics)\n        except Exception as e:\n            raise ValueError(f\"Invalid metrics dictionary: {e}\")\n    elif isinstance(metrics, DistributionConfig):\n        config = metrics\n    else:\n        raise TypeError(\"Invalid metrics dictionary\")\n\n    if config.n is None:\n        config.n = n\n    elif config.n != n:\n        print(\n            f\"Warning: `n` provided in both arguments ({n}) and config ({config.n}). \"\n            f\"Using value from arguments: {n}.\"\n        )\n        config.n = n\n\n    if not self.validate_moments(config.std, config.skewness, config.kurtosis):\n        raise ValueError(\"Invalid metrics dictionary\")\n\n    # --- 1. Initial Data Generation ---\n    num_outliers = int(config.n * config.outlier_ratio)\n    num_base = config.n - num_outliers\n\n    # Generate the main part of the distribution\n    # Use a non-central t-distribution to introduce initial skew\n    nc = config.skewness * (config.kurtosis / 3.0)  # Heuristic for non-centrality\n    df = max(\n        5, int(6 + 2 * (config.kurtosis - 3))\n    )  # Degrees of freedom influence kurtosis\n\n    base_data = stats.nct.rvs(df=df, nc=nc, size=num_base)\n\n    # Generate outliers to control the tails\n    if num_outliers &gt; 0:\n        # Generate outliers from a wider normal distribution\n        outlier_scale = config.std * (1.5 + config.kurtosis / 5.0)\n        outliers = np.random.normal(\n            loc=config.mean, scale=outlier_scale, size=num_outliers\n        )\n        data = np.concatenate([base_data, outliers])\n    else:\n        data = base_data\n\n    np.random.shuffle(data)\n\n    # Initial scaling to get closer to the target\n    data = config.mean + (data - np.mean(data)) * (\n        config.std / (np.std(data) + EPSILON)\n    )\n\n    # --- 2. Iterative Adjustment ---\n    for _ in range(max_iterations):\n        # Calculate current moments\n        current_mean = np.mean(data)\n        current_std = np.std(data, ddof=1)\n        current_median = np.median(data)\n\n        # Check for convergence on primary metrics (mean, std, median)\n        mean_ok = abs(current_mean - config.mean) &lt; (\n            abs(config.mean) * config.accuracy_threshold\n        )\n        std_ok = abs(current_std - config.std) &lt; (\n            config.std * config.accuracy_threshold\n        )\n        median_ok = abs(current_median - config.median) &lt; (\n            abs(config.median) * config.accuracy_threshold\n        )\n\n        if mean_ok and std_ok and median_ok:\n            break\n\n        # Adjustment for mean and std (rescale and shift)\n        if current_std &gt; EPSILON:\n            data = config.mean + (data - current_mean) * (config.std / current_std)\n\n        # Adjustment for median (gentle push towards the target)\n        median_diff = config.median - np.median(data)\n        # Apply a non-linear shift to move the median without ruining the mean/std too much\n        data += (\n            median_diff\n            * np.exp(-(((data - np.median(data)) / config.std) ** 2))\n            * 0.1\n        )\n\n    # --- 3. Finalization ---\n    # Final clipping and type casting\n    data = np.clip(data, config.min_val, config.max_val)\n\n    # Ensure min/max values are present\n    if data.min() &gt; config.min_val:\n        data[np.argmin(data)] = config.min_val\n\n    if data.max() &lt; config.max_val:\n        data[np.argmax(data)] = config.max_val\n\n    if int_flag:\n        data = np.round(data).astype(np.int64)\n\n    # Convert to desired output format\n    if output_as == \"pandas\":\n        return pd.Series(data, name=\"generated_values\")\n    elif output_as == \"polars\":\n        return pl.Series(name=\"generated_values\", values=data)\n    else:\n        return data\n</code></pre>"},{"location":"api/ds_tool/#ds_tool.DSTools.grubbs_test","title":"<code>grubbs_test(x, alpha=0.05)</code>  <code>staticmethod</code>","text":"<pre><code>    Performs Grubbs' test to identify a single outlier in a dataset.\n\n    This test assumes the data comes from a normally distributed population\n    and is designed to detect one outlier at a time.\n\n    Args:\n        x: A 1D NumPy array or Pandas Series of numerical data.\n        alpha: The significance level for the test (default: 0.05).\n\n    Returns:\n        A Pydantic model (GrubbsTestResult) containing the test results,\n        including a boolean flag for outlier detection and the outlier's value\n        and index if found.\n\n    Raises:\n        ValueError: If the input array has fewer than 3 elements.\n\n    Usage:\n        tools = DSTools()\n\n        # Test 1: Data with an outlier\n        print(\"\n</code></pre> <p>Testing on data WITH an outlier:\")             result1 = tools.grubbs_test(data_with_outlier)             print(f\"  Calculated G-statistic: {result1.g_calculated:.4f}\")             print(f\"  Critical G-value: {result1.g_critical:.4f}\")             if result1.is_outlier:                 print(f\"Outlier detected: The value is {result1.outlier_value:.2f} at index {result1.outlier_index}.\")             else:                 print(\"No outlier detected.\")</p> <pre><code>        # Test 2: Data without an outlier\n        print(\"\n</code></pre> <p>Testing on data WITHOUT an outlier:\")             result2 = tools.grubbs_test(data_without_outlier)             print(f\"  Calculated G-statistic: {result2.g_calculated:.4f}\")             print(f\"  Critical G-value: {result2.g_critical:.4f}\")             if result2.is_outlier:                 print(f\"Outlier detected, but shouldn't have been.\")             else:                 print(\"Correctly determined that there are no outliers.\")</p> Source code in <code>src\\ds_tool.py</code> <pre><code>@staticmethod\ndef grubbs_test(\n    x: Union[np.ndarray, pd.Series], alpha: float = 0.05\n) -&gt; GrubbsTestResult:\n    \"\"\"\n    Performs Grubbs' test to identify a single outlier in a dataset.\n\n    This test assumes the data comes from a normally distributed population\n    and is designed to detect one outlier at a time.\n\n    Args:\n        x: A 1D NumPy array or Pandas Series of numerical data.\n        alpha: The significance level for the test (default: 0.05).\n\n    Returns:\n        A Pydantic model (GrubbsTestResult) containing the test results,\n        including a boolean flag for outlier detection and the outlier's value\n        and index if found.\n\n    Raises:\n        ValueError: If the input array has fewer than 3 elements.\n\n    Usage:\n        tools = DSTools()\n\n        # Test 1: Data with an outlier\n        print(\"\\nTesting on data WITH an outlier:\")\n        result1 = tools.grubbs_test(data_with_outlier)\n        print(f\"  Calculated G-statistic: {result1.g_calculated:.4f}\")\n        print(f\"  Critical G-value: {result1.g_critical:.4f}\")\n        if result1.is_outlier:\n            print(f\"Outlier detected: The value is {result1.outlier_value:.2f} at index {result1.outlier_index}.\")\n        else:\n            print(\"No outlier detected.\")\n\n        # Test 2: Data without an outlier\n        print(\"\\nTesting on data WITHOUT an outlier:\")\n        result2 = tools.grubbs_test(data_without_outlier)\n        print(f\"  Calculated G-statistic: {result2.g_calculated:.4f}\")\n        print(f\"  Critical G-value: {result2.g_critical:.4f}\")\n        if result2.is_outlier:\n            print(f\"Outlier detected, but shouldn't have been.\")\n        else:\n            print(\"Correctly determined that there are no outliers.\")\n    \"\"\"\n    if not isinstance(x, (np.ndarray, pd.Series)):\n        raise TypeError(\"Input data x must be a NumPy array or Pandas Series.\")\n\n    # Grubbs' test requires at least 3 data points\n    n = len(x)\n    if n &lt; 3:\n        raise ValueError(\"Grubbs test requires at least 3 data points.\")\n\n    # Convert to numpy array for calculations\n    data = np.array(x)\n\n    # 1. Calculate the G-statistic\n    mean_x = np.mean(data)\n    std_x = np.std(data, ddof=1)  # Use sample standard deviation\n\n    if np.isclose(std_x, 0):\n        # If all values are the same, there are no outliers\n        return GrubbsTestResult(\n            is_outlier=False,\n            g_calculated=0.0,\n            g_critical=np.inf,  # Critical value is irrelevant here\n            outlier_value=None,\n            outlier_index=None,\n        )\n\n    max_deviation_index = np.argmax(np.abs(data - mean_x))\n    max_deviation_value = data[max_deviation_index]\n\n    numerator = np.abs(max_deviation_value - mean_x)\n    g_calculated = numerator / std_x\n\n    # 2. Calculate the critical G-value\n    t_value = stats.t.ppf(1 - alpha / (2 * n), n - 2)\n\n    numerator_critical = (n - 1) * t_value\n    denominator_critical = np.sqrt(n * (n - 2 + t_value**2))\n    g_critical = numerator_critical / denominator_critical\n\n    # 3. Compare and determine the result\n    is_outlier_detected = g_calculated &gt; g_critical\n\n    return GrubbsTestResult(\n        is_outlier=is_outlier_detected,\n        g_calculated=g_calculated,\n        g_critical=g_critical,\n        outlier_value=max_deviation_value if is_outlier_detected else None,\n        outlier_index=int(max_deviation_index) if is_outlier_detected else None,\n    )\n</code></pre>"},{"location":"api/ds_tool/#ds_tool.DSTools.labeling","title":"<code>labeling(df, col_name, order_flag=True)</code>","text":"<p>Encode categorical variables with optional ordering.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>Input DataFrame</p> required <code>col_name</code> <code>str</code> <p>Column name for transformation</p> required <code>order_flag</code> <code>bool</code> <p>Whether to apply ordering based on frequency</p> <code>True</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>DataFrame with encoded column</p> Usage <p>tools = DSTools() df = tools.labeling(df, 'category_column', True)</p> Source code in <code>src\\ds_tool.py</code> <pre><code>def labeling(\n    self, df: pd.DataFrame, col_name: str, order_flag: bool = True\n) -&gt; pd.DataFrame:\n    \"\"\"\n    Encode categorical variables with optional ordering.\n\n    Args:\n        df: Input DataFrame\n        col_name: Column name for transformation\n        order_flag: Whether to apply ordering based on frequency\n\n    Returns:\n        DataFrame with encoded column\n\n    Usage:\n         tools = DSTools()\n         df = tools.labeling(df, 'category_column', True)\n    \"\"\"\n    if col_name not in df.columns:\n        raise ValueError(f\"Column {col_name} not found in DataFrame\")\n\n    df_copy = df.copy()\n    unique_values = df_copy[col_name].unique()\n    value_index = dict(zip(unique_values, range(len(unique_values))))\n    print(f\"Set of unique indexes for &lt;{col_name}&gt;:\\n{value_index}\")\n\n    if order_flag:\n        counts = (\n            df_copy[col_name]\n            .value_counts(normalize=True)\n            .sort_values()\n            .index.tolist()\n        )\n        counts_dict = {val: i for i, val in enumerate(counts)}\n        encoder = OrdinalEncoder(categories=[list(counts_dict.keys())], dtype=int)\n    else:\n        encoder = OrdinalEncoder(dtype=int)\n\n    df_copy[col_name] = encoder.fit_transform(df_copy[[col_name]])\n    return df_copy\n</code></pre>"},{"location":"api/ds_tool/#ds_tool.DSTools.min_max_scale","title":"<code>min_max_scale(df, columns=None, const_val_fill=0.0)</code>  <code>staticmethod</code>","text":"<pre><code>    Scales specified columns of a DataFrame to the range [0, 1].\n\n    This method applies Min-Max scaling. If a column contains identical\n    values, it will be filled with `const_val_fill`. The original\n    DataFrame is not modified.\n\n    Args:\n        df: The input DataFrame (Pandas or Polars).\n        columns: A list of column names to scale. If None (default),\n                 all numerical columns will be scaled.\n        const_val_fill: The value to use for columns where all values\n                        are identical (to avoid division by zero).\n\n    Returns:\n        A new DataFrame with the specified columns scaled.\n\n    Usage:\n        tools = DSTools()\n        pd_scaled = tools.min_max_scale(pd_data, columns=['a', 'c'])\n        print(\"\n</code></pre> <p>Pandas DataFrame with scaled columns 'a' and 'c':\")             print(pd_scaled)</p> <pre><code>        pl_scaled = tools.min_max_scale(pl_data) # Scale all numeric columns\n        print(\"\n</code></pre> <p>Polars DataFrame with all numeric columns scaled:\")             print(pl_scaled)</p> <pre><code>        pl_scaled_half = tools.min_max_scale(pl_data, const_val_fill=0.5)\n        print(\"\n</code></pre> <p>Polars DataFrame with constant columns filled with 0.5:\")             print(pl_scaled_half)</p> Source code in <code>src\\ds_tool.py</code> <pre><code>@staticmethod\ndef min_max_scale(\n    df: Union[pd.DataFrame, pl.DataFrame],\n    columns: Optional[List[str]] = None,\n    const_val_fill: float = 0.0,\n) -&gt; Union[pd.DataFrame, pl.DataFrame]:\n    \"\"\"\n    Scales specified columns of a DataFrame to the range [0, 1].\n\n    This method applies Min-Max scaling. If a column contains identical\n    values, it will be filled with `const_val_fill`. The original\n    DataFrame is not modified.\n\n    Args:\n        df: The input DataFrame (Pandas or Polars).\n        columns: A list of column names to scale. If None (default),\n                 all numerical columns will be scaled.\n        const_val_fill: The value to use for columns where all values\n                        are identical (to avoid division by zero).\n\n    Returns:\n        A new DataFrame with the specified columns scaled.\n\n    Usage:\n        tools = DSTools()\n        pd_scaled = tools.min_max_scale(pd_data, columns=['a', 'c'])\n        print(\"\\nPandas DataFrame with scaled columns 'a' and 'c':\")\n        print(pd_scaled)\n\n        pl_scaled = tools.min_max_scale(pl_data) # Scale all numeric columns\n        print(\"\\nPolars DataFrame with all numeric columns scaled:\")\n        print(pl_scaled)\n\n        pl_scaled_half = tools.min_max_scale(pl_data, const_val_fill=0.5)\n        print(\"\\nPolars DataFrame with constant columns filled with 0.5:\")\n        print(pl_scaled_half)\n    \"\"\"\n    if isinstance(df, pd.DataFrame):\n        # --- Pandas Implementation ---\n        df_new = df.copy()\n\n        if columns is None:\n            # Select all numeric columns if none are specified\n            columns = df_new.select_dtypes(include=np.number).columns.tolist()\n\n        for col in columns:\n            if col not in df_new.columns:\n                print(f\"Warning: Column '{col}' not found in DataFrame. Skipping.\")\n                continue\n\n            min_val = df_new[col].min()\n            max_val = df_new[col].max()\n\n            if min_val == max_val:\n                df_new[col] = const_val_fill\n            else:\n                df_new[col] = (df_new[col] - min_val) / (max_val - min_val)\n\n        return df_new\n\n    elif isinstance(df, pl.DataFrame):\n        # --- Polars Implementation ---\n\n        if columns is None:\n            # Select all numeric columns\n            columns = [\n                col for col, dtype in df.schema.items() if dtype.is_numeric()\n            ]\n\n        expressions = []\n        for col in columns:\n            if col not in df.columns:\n                print(f\"Warning: Column '{col}' not found in DataFrame. Skipping.\")\n                continue\n\n            min_val = pl.col(col).min()\n            max_val = pl.col(col).max()\n\n            # Use a 'when/then/otherwise' expression for conditional logic\n            expr = (\n                pl.when(min_val == max_val)\n                .then(pl.lit(const_val_fill))\n                .otherwise((pl.col(col) - min_val) / (max_val - min_val))\n                .alias(col)  # Keep the original column name\n            )\n            expressions.append(expr)\n\n        return df.with_columns(expressions)\n\n    else:\n        raise TypeError(\"Input dataframe must be a Pandas or Polars DataFrame.\")\n</code></pre>"},{"location":"api/ds_tool/#ds_tool.DSTools.plot_confusion_matrix","title":"<code>plot_confusion_matrix(y_true, y_pred, class_labels=None, figsize=(8, 8), title='Confusion Matrix', cmap='Blues')</code>  <code>staticmethod</code>","text":"<p>Plots a clear and readable confusion matrix using seaborn.</p> <p>This method visualizes the performance of a classification model by showing the number of correct and incorrect predictions for each class.</p> <p>Parameters:</p> Name Type Description Default <code>y_true</code> <code>Union[ndarray, Series]</code> <p>Array-like of true labels.</p> required <code>y_pred</code> <code>Union[ndarray, Series]</code> <p>Array-like of predicted labels.</p> required <code>class_labels</code> <code>Optional[List[str]]</code> <p>Optional list of strings to use as labels for the axes.           If None, integer labels will be used.</p> <code>None</code> <code>figsize</code> <code>Tuple[int, int]</code> <p>Tuple specifying the figure size.</p> <code>(8, 8)</code> <code>title</code> <code>str</code> <p>The title for the plot.</p> <code>'Confusion Matrix'</code> <code>cmap</code> <code>str</code> <p>The colormap to use for the heatmap.</p> <code>'Blues'</code> <p>Usage:     tools = DSTools()</p> <pre><code>tools.plot_confusion_matrix(\n    y_true_binary,\n    y_pred_binary,\n    class_labels=['Negative (0)', 'Positive (1)'],\n    title='Binary Confusion Matrix'\n)\ntools.plot_confusion_matrix(\n    y_true_multi,\n    y_pred_multi,\n    class_labels=['Cat', 'Dog', 'Bird'],\n    title='Multi-Class Confusion Matrix'\n)\n</code></pre> Source code in <code>src\\ds_tool.py</code> <pre><code>@staticmethod\ndef plot_confusion_matrix(\n    y_true: Union[np.ndarray, pd.Series],\n    y_pred: Union[np.ndarray, pd.Series],\n    class_labels: Optional[List[str]] = None,\n    figsize: Tuple[int, int] = (8, 8),\n    title: str = \"Confusion Matrix\",\n    cmap: str = \"Blues\",\n):\n    \"\"\"\n    Plots a clear and readable confusion matrix using seaborn.\n\n    This method visualizes the performance of a classification model by showing\n    the number of correct and incorrect predictions for each class.\n\n    Args:\n        y_true: Array-like of true labels.\n        y_pred: Array-like of predicted labels.\n        class_labels: Optional list of strings to use as labels for the axes.\n                      If None, integer labels will be used.\n        figsize: Tuple specifying the figure size.\n        title: The title for the plot.\n        cmap: The colormap to use for the heatmap.\n    Usage:\n        tools = DSTools()\n\n        tools.plot_confusion_matrix(\n            y_true_binary,\n            y_pred_binary,\n            class_labels=['Negative (0)', 'Positive (1)'],\n            title='Binary Confusion Matrix'\n        )\n        tools.plot_confusion_matrix(\n            y_true_multi,\n            y_pred_multi,\n            class_labels=['Cat', 'Dog', 'Bird'],\n            title='Multi-Class Confusion Matrix'\n        )\n    \"\"\"\n    # 1. Calculate the confusion matrix\n    cm = confusion_matrix(y_true, y_pred)\n\n    # 2. Determine labels for the axes\n    if class_labels:\n        if len(class_labels) != cm.shape[0]:\n            raise ValueError(\n                f\"Number of class_labels ({len(class_labels)}) does not match \"\n                f\"number of classes in confusion matrix ({cm.shape[0]}).\"\n            )\n        labels = class_labels\n    else:\n        labels = np.arange(cm.shape[0])\n\n    # 3. Create the plot using seaborn's heatmap for better aesthetics\n    plt.style.use(\"seaborn-v0_8-whitegrid\")\n    fig, ax = plt.subplots(figsize=figsize)\n\n    sns.heatmap(\n        cm,\n        annot=True,  # Display the numbers in the cells\n        fmt=\"d\",  # Format numbers as integers\n        cmap=cmap,  # Use the specified colormap\n        xticklabels=labels,\n        yticklabels=labels,\n        ax=ax,  # Draw on our created axes\n        annot_kws={\"size\": 14},  # Increase annotation font size\n    )\n\n    # 4. Set titles and labels for clarity\n    ax.set_title(title, fontsize=16, pad=20)\n    ax.set_xlabel(\"Predicted Label\", fontsize=14)\n    ax.set_ylabel(\"True Label\", fontsize=14)\n\n    # Rotate tick labels for better readability if they are long\n    plt.xticks(rotation=45, ha=\"right\")\n    plt.yticks(rotation=0)\n\n    plt.tight_layout()\n    plt.show()\n</code></pre>"},{"location":"api/ds_tool/#ds_tool.DSTools.read_dataframes_from_zip","title":"<code>read_dataframes_from_zip(zip_filename, format='parquet', backend='polars')</code>  <code>staticmethod</code>","text":"<p>Reads one or more DataFrames from a ZIP archive.</p> <p>Parameters:</p> Name Type Description Default <code>zip_filename</code> <code>str</code> <p>The path to the ZIP archive.</p> required <code>format</code> <code>str</code> <p>The format of the files inside the ZIP ('parquet', 'csv').</p> <code>'parquet'</code> <code>backend</code> <code>str</code> <p>The library to use for reading ('polars' or 'pandas').</p> <code>'polars'</code> <p>Returns:</p> Type Description <code>Dict[str, Union[DataFrame, DataFrame]]</code> <p>A dictionary where keys are the filenames (without extension) and</p> <code>Dict[str, Union[DataFrame, DataFrame]]</code> <p>values are the loaded DataFrames.</p> Source code in <code>src\\ds_tool.py</code> <pre><code>@staticmethod\ndef read_dataframes_from_zip(\n    zip_filename: str, format: str = \"parquet\", backend: str = \"polars\"\n) -&gt; Dict[str, Union[pd.DataFrame, pl.DataFrame]]:\n    \"\"\"\n    Reads one or more DataFrames from a ZIP archive.\n\n    Args:\n        zip_filename: The path to the ZIP archive.\n        format: The format of the files inside the ZIP ('parquet', 'csv').\n        backend: The library to use for reading ('polars' or 'pandas').\n\n    Returns:\n        A dictionary where keys are the filenames (without extension) and\n        values are the loaded DataFrames.\n    \"\"\"\n    if backend not in [\"polars\", \"pandas\"]:\n        raise ValueError(\"`backend` must be 'polars' or 'pandas'.\")\n\n    loaded_dataframes = {}\n\n    with tempfile.TemporaryDirectory() as temp_dir:\n        with zipfile.ZipFile(zip_filename, \"r\") as zipf:\n            zipf.extractall(temp_dir)\n\n        extension = f\".{format}\"\n        for filename in os.listdir(temp_dir):\n            if filename.endswith(extension):\n                file_path = os.path.join(temp_dir, filename)\n                name_without_ext = os.path.splitext(filename)[0]\n\n                # --- Dispatch based on a chosen backend ---\n                if backend == \"polars\":\n                    df = (\n                        pl.read_parquet(file_path)\n                        if format == \"parquet\"\n                        else pl.read_csv(file_path)\n                    )\n                else:  # backend == 'pandas'\n                    if format == \"parquet\":\n                        df = pd.read_parquet(file_path)\n                    else:\n                        try:\n                            df = pd.read_csv(file_path, index_col=0)\n                        except (ValueError, IndexError):\n                            df = pd.read_csv(file_path)\n\n                loaded_dataframes[name_without_ext] = df\n\n    print(\n        f\"Successfully loaded {len(loaded_dataframes)} DataFrame(s) using {backend}.\"\n    )\n    return loaded_dataframes\n</code></pre>"},{"location":"api/ds_tool/#ds_tool.DSTools.remove_outliers_iqr","title":"<code>remove_outliers_iqr(df, column_name, config=None)</code>","text":"<p>Remove outliers using IQR (Inter Quartile Range) method.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>Input DataFrame</p> required <code>column_name</code> <code>str</code> <p>Target column name</p> required <code>config</code> <code>Optional[OutlierConfig]</code> <p>Configuration for outlier removal</p> <code>None</code> <p>Returns:</p> Type Description <code>Union[DataFrame, Tuple[DataFrame, float, float]]</code> <p>Modified DataFrame, optionally with outlier percentages</p> Usage <p>from ds_tool import DSTools, OutlierConfig tools = DSTools() config_custom = OutlierConfig(sigma=1.0, percentage=False) df_clean = tools.remove_outliers_iqr(df, 'target_column', config=config_custom) df_replaced, p_upper, p_lower = tools.remove_outliers_iqr(df, 'target_column')</p> Source code in <code>src\\ds_tool.py</code> <pre><code>def remove_outliers_iqr(\n    self, df: pd.DataFrame, column_name: str, config: Optional[OutlierConfig] = None\n) -&gt; Union[pd.DataFrame, Tuple[pd.DataFrame, float, float]]:\n    \"\"\"\n    Remove outliers using IQR (Inter Quartile Range) method.\n\n    Args:\n        df: Input DataFrame\n        column_name: Target column name\n        config: Configuration for outlier removal\n\n    Returns:\n        Modified DataFrame, optionally with outlier percentages\n\n    Usage:\n         from ds_tool import DSTools, OutlierConfig\n         tools = DSTools()\n         config_custom = OutlierConfig(sigma=1.0, percentage=False)\n         df_clean = tools.remove_outliers_iqr(df, 'target_column', config=config_custom)\n         df_replaced, p_upper, p_lower = tools.remove_outliers_iqr(df, 'target_column')\n    \"\"\"\n    if config is None:\n        config = OutlierConfig()\n\n    if column_name not in df.columns:\n        raise ValueError(f\"Column {column_name} not found in DataFrame\")\n\n    df_copy = df.copy()\n    target = df_copy[column_name]\n\n    q1 = target.quantile(0.25)\n    q3 = target.quantile(0.75)\n    iqr = q3 - q1\n    iqr_lower = q1 - config.sigma * iqr\n    iqr_upper = q3 + config.sigma * iqr\n\n    outliers_upper = target &gt; iqr_upper\n    outliers_lower = target &lt; iqr_lower\n\n    if config.change_remove:\n        df_copy.loc[outliers_upper, column_name] = iqr_upper\n        df_copy.loc[outliers_lower, column_name] = iqr_lower\n    else:\n        df_copy = df_copy[~(outliers_upper | outliers_lower)]\n\n    if config.percentage:\n        percent_upper = round(outliers_upper.sum() / len(df) * 100, 2)\n        percent_lower = round(outliers_lower.sum() / len(df) * 100, 2)\n        return df_copy, percent_upper, percent_lower\n\n    return df_copy\n</code></pre>"},{"location":"api/ds_tool/#ds_tool.DSTools.save_dataframes_to_zip","title":"<code>save_dataframes_to_zip(dataframes, zip_filename, format='parquet', save_index=False)</code>  <code>staticmethod</code>","text":"<pre><code>    Saves one or more Pandas or Polars DataFrames into a single ZIP archive.\n\n    Args:\n        dataframes: A dictionary where keys are the desired filenames (without\n                    extension) and values are the Pandas or Polars DataFrames.\n        zip_filename: The path for the output ZIP archive.\n        format: The format to save the data in ('parquet', 'csv').\n        save_index: For Pandas DataFrames, whether to save the index.\n                    (Ignored for Polars).\n\n    Usage:\n        tools = DSTools()\n        dfs_to_save = {\n            'pandas_data': pd_df,\n            'polars_data': pl_df\n        }\n        zip_path = 'mixed_data_archive.zip'\n        print(\"\n</code></pre> <p>--- Saving mixed DataFrames ---\")             tools.save_dataframes_to_zip(dfs_to_save, zip_path, format='parquet', save_index=True)</p> <pre><code>        print(\"\n</code></pre> <p>--- Reading back with Polars backend ---\")             loaded_with_polars = tools.read_dataframes_from_zip(zip_path, format='parquet', backend='polars')             print(\"DataFrame 'pandas_data' loaded by Polars:\")             print(loaded_with_polars['pandas_data']) # The index will be lost because Polars does not have it.</p> <pre><code>        print(\"\n</code></pre> <p>--- Reading back with Pandas backend ---\")             loaded_with_pandas = tools.read_dataframes_from_zip(zip_path, format='parquet', backend='pandas')             print(\"DataFrame 'pandas_data' loaded by Pandas:\")             print(loaded_with_pandas['pandas_data']) # The index will be restored</p> <pre><code>        if os.path.exists(zip_path):\n        os.remove(zip_path)\n</code></pre> Source code in <code>src\\ds_tool.py</code> <pre><code>@staticmethod\ndef save_dataframes_to_zip(\n    dataframes: Dict[str, Union[pd.DataFrame, pl.DataFrame]],\n    zip_filename: str,\n    format: str = \"parquet\",\n    save_index: bool = False,\n):\n    \"\"\"\n    Saves one or more Pandas or Polars DataFrames into a single ZIP archive.\n\n    Args:\n        dataframes: A dictionary where keys are the desired filenames (without\n                    extension) and values are the Pandas or Polars DataFrames.\n        zip_filename: The path for the output ZIP archive.\n        format: The format to save the data in ('parquet', 'csv').\n        save_index: For Pandas DataFrames, whether to save the index.\n                    (Ignored for Polars).\n\n    Usage:\n        tools = DSTools()\n        dfs_to_save = {\n            'pandas_data': pd_df,\n            'polars_data': pl_df\n        }\n        zip_path = 'mixed_data_archive.zip'\n        print(\"\\n--- Saving mixed DataFrames ---\")\n        tools.save_dataframes_to_zip(dfs_to_save, zip_path, format='parquet', save_index=True)\n\n        print(\"\\n--- Reading back with Polars backend ---\")\n        loaded_with_polars = tools.read_dataframes_from_zip(zip_path, format='parquet', backend='polars')\n        print(\"DataFrame 'pandas_data' loaded by Polars:\")\n        print(loaded_with_polars['pandas_data']) # The index will be lost because Polars does not have it.\n\n        print(\"\\n--- Reading back with Pandas backend ---\")\n        loaded_with_pandas = tools.read_dataframes_from_zip(zip_path, format='parquet', backend='pandas')\n        print(\"DataFrame 'pandas_data' loaded by Pandas:\")\n        print(loaded_with_pandas['pandas_data']) # The index will be restored\n\n        if os.path.exists(zip_path):\n        os.remove(zip_path)\n    \"\"\"\n    if not isinstance(dataframes, dict):\n        raise TypeError(\"`dataframes` must be a dictionary of {filename: df}.\")\n\n    with tempfile.TemporaryDirectory() as temp_dir:\n        file_paths = []\n\n        for name, df in dataframes.items():\n            safe_name = re.sub(r'[\\\\/*?:\"&lt;&gt;|]', \"_\", name)\n            file_path = os.path.join(temp_dir, f\"{safe_name}.{format}\")\n            file_paths.append(file_path)\n\n            # --- Dispatch based on DataFrame type ---\n            if isinstance(df, pd.DataFrame):\n                if format == \"parquet\":\n                    df.to_parquet(file_path, index=save_index, engine=\"fastparquet\")\n                elif format == \"csv\":\n                    df.to_csv(file_path, index=save_index)\n                else:\n                    raise ValueError(f\"Unsupported format: '{format}'.\")\n            elif isinstance(df, pl.DataFrame):\n                if format == \"parquet\":\n                    df.write_parquet(file_path)\n                elif format == \"csv\":\n                    df.write_csv(file_path)\n                else:\n                    raise ValueError(f\"Unsupported format: '{format}'.\")\n            else:\n                raise TypeError(f\"Unsupported DataFrame type: {type(df)}\")\n\n        # Create the ZIP archive\n        with zipfile.ZipFile(zip_filename, \"w\", zipfile.ZIP_DEFLATED) as zipf:\n            for path in file_paths:\n                zipf.write(path, os.path.basename(path))\n\n    print(f\"Successfully saved {len(dataframes)} DataFrame(s) to {zip_filename}\")\n</code></pre>"},{"location":"api/ds_tool/#ds_tool.DSTools.sparse_calc","title":"<code>sparse_calc(df)</code>","text":"<p>Calculate sparsity level as coefficient.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>Input DataFrame</p> required <p>Returns:</p> Type Description <code>float</code> <p>Sparsity coefficient as percentage</p> Usage <p>tools = DSTools() sparsity = tools.sparse_calc(df)</p> Source code in <code>src\\ds_tool.py</code> <pre><code>def sparse_calc(self, df: pd.DataFrame) -&gt; float:\n    \"\"\"\n    Calculate sparsity level as coefficient.\n\n    Args:\n        df: Input DataFrame\n\n    Returns:\n        Sparsity coefficient as percentage\n\n    Usage:\n         tools = DSTools()\n         sparsity = tools.sparse_calc(df)\n    \"\"\"\n    sparse_coef = round(df.apply(pd.arrays.SparseArray).sparse.density * 100, 2)\n    print(f\"Level of sparsity = {sparse_coef} %\")\n\n    return sparse_coef\n</code></pre>"},{"location":"api/ds_tool/#ds_tool.DSTools.stat_normal_testing","title":"<code>stat_normal_testing(check_object, describe_flag=False)</code>","text":"<p>Perform D'Agostino's K\u00b2 test for normality testing.</p> <p>Parameters:</p> Name Type Description Default <code>check_object</code> <code>Union[DataFrame, Series]</code> <p>Input data (DataFrame or Series)</p> required <code>describe_flag</code> <code>bool</code> <p>Whether to show descriptive statistics</p> <code>False</code> Usage <p>tools = DSTools()</p> <p>tools.stat_normal_testing(data, describe_flag=True)</p> Source code in <code>src\\ds_tool.py</code> <pre><code>def stat_normal_testing(\n    self, check_object: Union[pd.DataFrame, pd.Series], describe_flag: bool = False\n) -&gt; None:\n    \"\"\"\n    Perform D'Agostino's K\u00b2 test for normality testing.\n\n    Args:\n        check_object: Input data (DataFrame or Series)\n        describe_flag: Whether to show descriptive statistics\n\n    Usage:\n         tools = DSTools()\n\n         tools.stat_normal_testing(data, describe_flag=True)\n    \"\"\"\n    if isinstance(check_object, pd.DataFrame) and len(check_object.columns) == 1:\n        check_object = check_object.iloc[:, 0]\n\n    # Perform normality test\n    stat, p_value = stats.normaltest(check_object)\n    print(f\"Statistics = {stat:.3f}, p = {p_value:.3f}\")\n\n    alpha = 0.05\n    if p_value &gt; alpha:\n        print(\"Data looks Gaussian (fail to reject H0). Data is normal\")\n    else:\n        print(\"Data does not look Gaussian (reject H0). Data is not normal\")\n\n    # Calculate kurtosis and skewness\n    kurtosis_val = stats.kurtosis(check_object)\n    skewness_val = stats.skew(check_object)\n\n    print(f\"\\nKurtosis: {kurtosis_val:.3f}\")\n    if kurtosis_val &gt; 0:\n        print(\"Distribution has heavier tails than normal\")\n    elif kurtosis_val &lt; 0:\n        print(\"Distribution has lighter tails than normal\")\n    else:\n        print(\"Distribution has normal tail weight\")\n\n    print(f\"\\nSkewness: {skewness_val:.3f}\")\n    if -0.5 &lt;= skewness_val &lt;= 0.5:\n        print(\"Data are fairly symmetrical\")\n    elif skewness_val &lt; -1 or skewness_val &gt; 1:\n        print(\"Data are highly skewed\")\n    else:\n        print(\"Data are moderately skewed\")\n\n    # Visualization\n    sns.displot(check_object, bins=30)\n    plt.title(\"Distribution of the data\")\n    plt.show()\n\n    if describe_flag:\n        print(\"\\nDescriptive Statistics:\")\n        print(check_object.describe())\n\n        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 6))\n\n        ax1.hist(check_object, bins=50, edgecolor=\"black\")\n        ax1.set_title(\"Histogram\")\n        ax1.set_xlabel(\"Values\")\n        ax1.set_ylabel(\"Frequency\")\n\n        stats.probplot(check_object, dist=\"norm\", plot=ax2)\n        ax2.set_title(\"Q-Q Plot\")\n\n        plt.tight_layout()\n        plt.show()\n</code></pre>"},{"location":"api/ds_tool/#ds_tool.DSTools.test_stationarity","title":"<code>test_stationarity(check_object, print_results_flag=True, len_window=30)</code>","text":"<p>Perform Dickey-Fuller test for stationarity testing.</p> <p>Parameters:</p> Name Type Description Default <code>check_object</code> <code>Series</code> <p>Input time series data</p> required <code>print_results_flag</code> <code>bool</code> <p>Whether to print detailed results</p> <code>True</code> <code>len_window</code> <code>int</code> <p>length of a window, default is 30</p> <code>30</code> Usage <p>tools = DSTools() tools.test_stationarity(time_series, print_results_flag=True)</p> Source code in <code>src\\ds_tool.py</code> <pre><code>def test_stationarity(\n    self,\n    check_object: pd.Series,\n    print_results_flag: bool = True,\n    len_window: int = 30,\n) -&gt; None:\n    \"\"\"\n    Perform Dickey-Fuller test for stationarity testing.\n\n    Args:\n        check_object: Input time series data\n        print_results_flag: Whether to print detailed results\n        len_window: length of a window, default is 30\n\n    Usage:\n         tools = DSTools()\n         tools.test_stationarity(time_series, print_results_flag=True)\n    \"\"\"\n    # Calculate rolling statistics\n    rolling_mean = check_object.rolling(window=len_window).mean()\n    rolling_std = check_object.rolling(window=len_window).std()\n\n    # Plot rolling statistics\n    fig, ax = plt.subplots(figsize=(12, 6))\n    ax.plot(check_object, color=\"blue\", label=\"Original\", linewidth=2)\n    ax.plot(rolling_mean, color=\"red\", label=\"Rolling Mean\", linewidth=2)\n    ax.plot(rolling_std, color=\"black\", label=\"Rolling Std\", linewidth=2)\n\n    ax.legend(loc=\"upper left\")\n    ax.set_title(\"Rolling Mean &amp; Standard Deviation\")\n    ax.grid(True, alpha=0.3)\n    plt.tight_layout()\n    plt.show()\n    plt.close(fig)\n\n    # Perform Dickey-Fuller test\n    adf_result = adfuller(check_object, autolag=\"AIC\")\n    adf_output = pd.Series(\n        adf_result[0:4],\n        index=[\n            \"Test Statistic\",\n            \"p-value\",\n            \"Lags Used\",\n            \"Number of Observations Used\",\n        ],\n    )\n\n    for key, value in adf_result[4].items():\n        adf_output[f\"Critical Value ({key})\"] = value\n\n    if print_results_flag:\n        print(\"Results of Dickey-Fuller Test:\")\n        print(adf_output)\n\n    # Interpret results\n    if adf_output[\"p-value\"] &lt;= 0.05:\n        print(\"\\nData does not have a unit root. Data is STATIONARY!\")\n    else:\n        print(\"\\nData has a unit root. Data is NON-STATIONARY!\")\n</code></pre>"},{"location":"api/ds_tool/#ds_tool.DSTools.trials_res_df","title":"<code>trials_res_df(study_trials, metric)</code>","text":"<p>Aggregate Optuna optimization trials as DataFrame.</p> <p>Parameters:</p> Name Type Description Default <code>study_trials</code> <code>List[Any]</code> <p>List of Optuna trials (study.trials)</p> required <code>metric</code> <code>str</code> <p>Metric name for sorting (e.g., 'MCC', 'F1')</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>DataFrame with aggregated trial results</p> Usage <p>tools = DSTools() results = tools.trials_res_df(study.trials, 'MCC')</p> Source code in <code>src\\ds_tool.py</code> <pre><code>def trials_res_df(self, study_trials: List[Any], metric: str) -&gt; pd.DataFrame:\n    \"\"\"\n    Aggregate Optuna optimization trials as DataFrame.\n\n    Args:\n        study_trials: List of Optuna trials (study.trials)\n        metric: Metric name for sorting (e.g., 'MCC', 'F1')\n\n    Returns:\n        DataFrame with aggregated trial results\n\n    Usage:\n         tools = DSTools()\n         results = tools.trials_res_df(study.trials, 'MCC')\n    \"\"\"\n    df_results = pd.DataFrame()\n\n    for trial in study_trials:\n        if trial.value is None:\n            continue\n\n        trial_data = pd.DataFrame.from_dict(trial.params, orient=\"index\").T\n        trial_data.insert(0, metric, trial.value)\n\n        if trial.datetime_complete and trial.datetime_start:\n            duration = (\n                trial.datetime_complete - trial.datetime_start\n            ).total_seconds()\n            trial_data[\"Duration\"] = duration\n\n        df_results = pd.concat([df_results, trial_data], ignore_index=True)\n\n    df_results = df_results.sort_values(metric, ascending=False)\n\n    for col in df_results.columns:\n        if col not in [metric, \"Duration\"]:\n            df_results[col] = pd.to_numeric(df_results[col], errors=\"coerce\")\n\n    return df_results\n</code></pre>"},{"location":"api/ds_tool/#ds_tool.DSTools.validate_moments","title":"<code>validate_moments(std, skewness, kurtosis)</code>  <code>staticmethod</code>","text":"<p>Validate that statistical moments are physically possible. A key property is that kurtosis must be greater than or equal to the square of skewness minus 2.</p> <p>Parameters:</p> Name Type Description Default <code>std</code> <code>float</code> <p>Standard deviation</p> required <code>skewness</code> <code>float</code> <p>Skewness value</p> required <code>kurtosis</code> <code>float</code> <p>Kurtosis value</p> required <p>Returns:</p> Type Description <code>bool</code> <p>True if moments are valid, False otherwise</p> Usage <p>tools = DSTools() is_valid = tools.validate_moments(1.0, 0.5, 3.0)</p> Source code in <code>src\\ds_tool.py</code> <pre><code>@staticmethod\ndef validate_moments(std: float, skewness: float, kurtosis: float) -&gt; bool:\n    \"\"\"\n    Validate that statistical moments are physically possible.\n    A key property is that kurtosis must be greater than or equal to\n    the square of skewness minus 2.\n\n    Args:\n        std: Standard deviation\n        skewness: Skewness value\n        kurtosis: Kurtosis value\n\n    Returns:\n        True if moments are valid, False otherwise\n\n    Usage:\n         tools = DSTools()\n         is_valid = tools.validate_moments(1.0, 0.5, 3.0)\n    \"\"\"\n    return std &gt; 0 and kurtosis &gt;= (skewness**2 - 2)\n</code></pre>"}]}